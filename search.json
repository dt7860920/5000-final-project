[
  {
    "objectID": "data_collection_cleaning/main.html",
    "href": "data_collection_cleaning/main.html",
    "title": "Data Collection from PGA Website",
    "section": "",
    "text": "This file is used to scrape data from PGA official website\n\n\nCode\nimport time\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\n\n\nCode\ndef get_headers(soup):\n    #Collect column names from PGA website\n    #Start with empty list\n    headers = []\n\n    # \"Rounds\" header\n    rounds = soup.find_all(class_=\"rounds hidden-small hidden-medium\")[0].get_text(strip=True)\n    headers.append(rounds)\n\n    # Other stat headers\n    stat_headers = soup.find_all(class_=\"col-stat hidden-small hidden-medium\")\n    for header in stat_headers:\n        headers.append(header.get_text(strip=True))\n\n    return headers\n\n\ndef get_players(soup):\n    # Extract player names from a stats page.\n    # Start with empty name list\n    player_list = []\n\n    # We start collecting from 1 because first one is usually not a player\n    players = soup.select(\"td a\")[1:]\n    for player in players:\n        player_list.append(player.get_text(strip=True))\n\n    return player_list\n\n\ndef get_stats(soup, categories):\n    # Collect stats from website\n    # Get all numeric data\n    stats = soup.find_all(class_=\"hidden-small hidden-medium\")\n\n    stat_list = []\n    # Go through the loop to organize data based on how many column of stats we have\n    for i in range(0, len(stats) - categories + 1, categories):\n        row = []\n        for j in range(categories):\n            row.append(stats[i + j].get_text(strip=True))\n        stat_list.append(row)\n\n    return stat_list\n\ndef stats_dict(players, stats):\n    # Combine and convert the name and stats list into one, and use index to pull the matching data\n    player_dict = {}\n    for i, player in enumerate(players):\n        player_dict[player] = stats[i]\n    return player_dict"
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "report",
    "section": "",
    "text": "Professional golf has gone through a tremendous evolution over the past 20 years. Technical development in equipment, training method, and analytics have transformed the sport closer to a data driven decision making process, rather than feeling and muscle memory. Many may wonder: What actually drives performance on the PGA Tour?\nMost amateur golfers often focus on distance, consistency, and precision without an accurate measure. However, data from PGA Tour suggest some more complex metrics. The stroke gain (SG) is a golf statistic that shows how many strokes you gained or lost on any given shot compared to an average player during your round of golf.\nThis project analyzes PGA Tour player performance from 2007 to 2022, by combining two publicly available professional datasets into an integrated dataframe. The goal is to use data analysis tools to answer key performance questions and unveil how modern professional golf has evolved.\n\n\n\nClearly list the key questions your analysis answers, such as:\n\n\nHow have key PGA Tour metrics (driving distance, SG metrics) evolved from 2007–2022?\nAre there identifiable types of PGA Tour players using unsupervised learning?\nWhich performance metrics best predict scoring average on the PGA Tour?\nWhat distinguishes winning players from the rest of the field?\nIs modern success driven more by strokes gained tee-to-green or putting\nDoes distance directly contributes to better score or titles won when compared to stroke gain metrics?"
  },
  {
    "objectID": "report.html#introduction",
    "href": "report.html#introduction",
    "title": "report",
    "section": "",
    "text": "Professional golf has gone through a tremendous evolution over the past 20 years. Technical development in equipment, training method, and analytics have transformed the sport closer to a data driven decision making process, rather than feeling and muscle memory. Many may wonder: What actually drives performance on the PGA Tour?\nMost amateur golfers often focus on distance, consistency, and precision without an accurate measure. However, data from PGA Tour suggest some more complex metrics. The stroke gain (SG) is a golf statistic that shows how many strokes you gained or lost on any given shot compared to an average player during your round of golf.\nThis project analyzes PGA Tour player performance from 2007 to 2022, by combining two publicly available professional datasets into an integrated dataframe. The goal is to use data analysis tools to answer key performance questions and unveil how modern professional golf has evolved.\n\n\n\nClearly list the key questions your analysis answers, such as:\n\n\nHow have key PGA Tour metrics (driving distance, SG metrics) evolved from 2007–2022?\nAre there identifiable types of PGA Tour players using unsupervised learning?\nWhich performance metrics best predict scoring average on the PGA Tour?\nWhat distinguishes winning players from the rest of the field?\nIs modern success driven more by strokes gained tee-to-green or putting\nDoes distance directly contributes to better score or titles won when compared to stroke gain metrics?"
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised Learning\n\n\nSetup\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import root_mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n    accuracy_score,\n    roc_auc_score,\n    classification_report,\n)\n\n\n\n\nLoad data\n\n\nCode\npga = pd.read_csv(\"data/processed-data/pga_cleaned.csv\")\npga.head()\npga.columns\n\n\nIndex(['name', 'year', 'country', 'scoring', 'drive_distance', 'gir_pct',\n       'sg_p', 'sg_ttg', 'sg_t', 'top_10', 'win'],\n      dtype='object')\n\n\n\n\nRegression (Predict Score)\n\n\nCode\nreg_features = [\n  \"drive_distance\",\n  \"gir_pct\",\n  \"sg_p\",\n  \"sg_ttg\",\n]\nreg_target = \"scoring\"\n\nreg_df = pga[reg_features + [reg_target]].dropna()\nX_reg = reg_df[reg_features]\ny_reg = reg_df[reg_target]\n\nX_reg.shape, y_reg.shape\n\n\n((2747, 4), (2747,))\n\n\n\n\nTraining and testing split (Predict Score)\n\n\nCode\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\nX_reg, y_reg, test_size=0.2, random_state=42\n)\n\nscaler_reg = StandardScaler()\nX_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\nX_test_reg_scaled = scaler_reg.transform(X_test_reg)\n\n\n\n\nLinear Regression (Predict Score)\n\n\nCode\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_reg_scaled, y_train_reg)\n\ny_pred_lin = lin_reg.predict(X_test_reg_scaled)\n\nrmse_lin = root_mean_squared_error(y_test_reg, y_pred_lin)\nr2_lin = r2_score(y_test_reg, y_pred_lin)\n\nprint(\"Linear Regression (Predict Score)\")\nprint(\" RMSE:\", rmse_lin)\nprint(\" R^2 :\", r2_lin)\n\n\nLinear Regression (Predict Score)\n RMSE: 0.1919919438714076\n R^2 : 0.9098003470553254\n\n\n\n\nRandom Forest (Predict Score)\n\n\nCode\nrf_reg = RandomForestRegressor(\n  n_estimators=300,\n  random_state=42,\n  max_depth=None,\n  n_jobs=-1\n)\nrf_reg.fit(X_train_reg, y_train_reg)\n\ny_pred_rf = rf_reg.predict(X_test_reg)\n\nrmse_rf = root_mean_squared_error(y_test_reg, y_pred_rf)\nr2_rf = r2_score(y_test_reg, y_pred_rf)\n\nprint(\"Random Forest Regression (Predict Score)\")\nprint(\"  RMSE:\", rmse_rf)\nprint(\"  R^2 :\", r2_rf)\n\n\nRandom Forest Regression (Predict Score)\n  RMSE: 0.2057418971539437\n  R^2 : 0.8964179897284077\n\n\n\n\nFeature importance (Predict Score)\n\n\nCode\nimportances_reg = pd.Series(\nrf_reg.feature_importances_,\nindex=reg_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_reg.values, y=importances_reg.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Score)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_scoring.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_reg\n\n\n\n\n\n\n\n\n\nsg_ttg            0.726565\nsg_p              0.233879\ngir_pct           0.020768\ndrive_distance    0.018789\ndtype: float64\n\n\n\n\nClassification Model (Predict Win)\n\n\nCode\npga[\"has_win\"] = (pga[\"win\"] &gt; 0).astype(int)\n\nclf_features = [\n\"drive_distance\",\n\"gir_pct\",\n\"sg_p\",\n\"sg_ttg\",\n]\n\nclf_df = pga[clf_features + [\"has_win\"]].dropna()\nX_clf = clf_df[clf_features]\ny_clf = clf_df[\"has_win\"]\n\nX_clf.shape, y_clf.value_counts()\n\n\n((2747, 4),\n has_win\n 0    2410\n 1     337\n Name: count, dtype: int64)\n\n\n\n\nTraining and testing split (Predict Win)\n\n\nCode\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\nX_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n)\n\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n\n\n\n\nLogistic Regression (Predict Win)\n\n\nCode\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train_clf_scaled, y_train_clf)\n\ny_pred_log = log_reg.predict(X_test_clf_scaled)\ny_prob_log = log_reg.predict_proba(X_test_clf_scaled)[:, 1]\n\nacc_log = accuracy_score(y_test_clf, y_pred_log)\nauc_log = roc_auc_score(y_test_clf, y_prob_log)\n\nprint(\"Logistic Regression (Predict Win)\")\nprint(\"  Accuracy:\", acc_log)\nprint(\"  ROC AUC :\", auc_log)\nprint(classification_report(y_test_clf, y_pred_log))\n\n\nLogistic Regression (Predict Win)\n  Accuracy: 0.8818181818181818\n  ROC AUC : 0.7675906183368869\n              precision    recall  f1-score   support\n\n           0       0.88      1.00      0.94       483\n           1       0.75      0.04      0.08        67\n\n    accuracy                           0.88       550\n   macro avg       0.82      0.52      0.51       550\nweighted avg       0.87      0.88      0.83       550\n\n\n\n\n\nRandom Forest (Predict Win)\n\n\nCode\nrf_clf = RandomForestClassifier(\nn_estimators=400,\nrandom_state=42,\nmax_depth=None,\nn_jobs=-1\n)\nrf_clf.fit(X_train_clf, y_train_clf)\n\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_prob_rf_clf = rf_clf.predict_proba(X_test_clf)[:, 1]\n\nacc_rf = accuracy_score(y_test_clf, y_pred_rf_clf)\nauc_rf = roc_auc_score(y_test_clf, y_prob_rf_clf)\n\nprint(\"Random Forest Classifier (Predict Win)\")\nprint(\"  Accuracy:\", acc_rf)\nprint(\"  ROC AUC :\", auc_rf)\nprint(classification_report(y_test_clf, y_pred_rf_clf))\n\n\nRandom Forest Classifier (Predict Win)\n  Accuracy: 0.88\n  ROC AUC : 0.7098668149933561\n              precision    recall  f1-score   support\n\n           0       0.89      0.99      0.94       483\n           1       0.54      0.10      0.17        67\n\n    accuracy                           0.88       550\n   macro avg       0.71      0.55      0.56       550\nweighted avg       0.85      0.88      0.84       550\n\n\n\n#Feature importance (Predict Win)\n\n\nCode\nimportances_clf = pd.Series(\nrf_clf.feature_importances_,\nindex=clf_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_clf.values, y=importances_clf.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Win)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_wins.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_clf\n\n\n\n\n\n\n\n\n\nsg_ttg            0.298724\ngir_pct           0.239733\ndrive_distance    0.231139\nsg_p              0.230404\ndtype: float64"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "EDA\n\n\nLoad Data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npath = \"data/processed-data/pga_cleaned.csv\"\npga = pd.read_csv(path)\n\n\n\n\nDistribution of Scoring Average\n\n\nCode\nyear_summary = (\n    pga.groupby(\"year\")[\"scoring\"]\n       .mean()\n       .reset_index()\n)\n\nplt.figure(figsize=(10, 5))\nplt.plot(year_summary[\"year\"], year_summary[\"scoring\"], marker=\"o\")\nplt.gca().invert_yaxis()  \nplt.xlabel(\"Year\")\nplt.ylabel(\"Average Scoring\")\nplt.title(\"Trend in PGA Tour Scoring Average Over Time\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nyearly_scores = pga.groupby(\"year\")[\"scoring\"].mean().round(3)\nyearly_scores.reset_index().rename(columns={\"scoring\": \"avg_scoring\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\navg_scoring\n\n\n\n\n0\n2007\n70.936\n\n\n1\n2008\n70.900\n\n\n2\n2009\n70.865\n\n\n3\n2010\n70.980\n\n\n4\n2011\n70.864\n\n\n5\n2012\n70.883\n\n\n6\n2013\n70.919\n\n\n7\n2014\n70.835\n\n\n8\n2015\n70.950\n\n\n9\n2016\n70.993\n\n\n10\n2017\n70.910\n\n\n11\n2018\n70.618\n\n\n12\n2019\n70.757\n\n\n13\n2020\n70.637\n\n\n14\n2021\n70.794\n\n\n15\n2022\n70.638\n\n\n\n\n\n\n\n\n\nScoring Average Variance\n\n\nCode\nplt.figure(figsize=(12,6))\nsns.boxplot(data=pga, x=\"year\", y=\"scoring\")\nplt.xticks(rotation=45)\nplt.title(\"Distribution of Scoring Averages by Year\")\nplt.show()\n\nyearly_spread = (\n    pga.groupby(\"year\")[\"scoring\"]\n       .agg([\"mean\", \"std\", \"min\", \"max\"])\n       .assign(range=lambda x: x[\"max\"] - x[\"min\"])\n)\n\nyearly_spread\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nstd\nmin\nmax\nrange\n\n\nyear\n\n\n\n\n\n\n\n\n\n2007\n70.936354\n0.748141\n67.794\n72.933\n5.139\n\n\n2008\n70.899655\n0.678822\n69.117\n73.203\n4.086\n\n\n2009\n70.864588\n0.692765\n68.052\n73.114\n5.062\n\n\n2010\n70.979656\n0.650667\n69.606\n73.070\n3.464\n\n\n2011\n70.863659\n0.670893\n68.861\n72.975\n4.114\n\n\n2012\n70.882577\n0.695553\n68.873\n72.571\n3.698\n\n\n2013\n70.919318\n0.700894\n68.893\n72.590\n3.697\n\n\n2014\n70.835375\n0.644715\n68.827\n72.593\n3.766\n\n\n2015\n70.949525\n0.644656\n68.938\n72.533\n3.595\n\n\n2016\n70.993489\n0.673068\n69.172\n74.262\n5.090\n\n\n2017\n70.910079\n0.684389\n68.846\n72.803\n3.957\n\n\n2018\n70.618176\n0.628434\n68.698\n72.182\n3.484\n\n\n2019\n70.756657\n0.566774\n69.057\n72.343\n3.286\n\n\n2020\n70.636875\n0.632075\n68.978\n72.022\n3.044\n\n\n2021\n70.794455\n0.551566\n69.300\n72.193\n2.893\n\n\n2022\n70.638064\n0.588539\n68.670\n72.038\n3.368\n\n\n\n\n\n\n\n\n\nDriving Distance Distribution\n\n\nCode\ndistance_trend = (\n    pga.groupby(\"year\")[\"drive_distance\"]\n       .mean()\n       .reset_index()\n)\n\nplt.figure(figsize=(10,5))\nplt.plot(distance_trend[\"year\"], distance_trend[\"drive_distance\"], marker=\"o\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Avg Driving Distance (yards)\")\nplt.title(\"Trend in Driving Distance Over Time\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSG_TTG Distribution\n\n\nCode\nsgttg_trend = (\n    pga.groupby(\"year\")[\"sg_ttg\"]\n       .mean()\n       .reset_index()\n       .dropna()\n)\n\nplt.figure(figsize=(10, 5))\nplt.plot(sgttg_trend[\"year\"], sgttg_trend[\"sg_ttg\"], marker=\"o\")\nplt.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.5)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Average SG Tee-to-Green\")\nplt.title(\"Average SG_TTG by Year\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSG_P Distribution\n\n\nCode\nsgp_trend = (\n    pga.groupby(\"year\")[\"sg_p\"]\n       .mean()\n       .reset_index()\n       .dropna()\n)\n\nplt.figure(figsize=(10, 5))\nplt.plot(sgp_trend[\"year\"], sgp_trend[\"sg_p\"], marker=\"o\")\nplt.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.5)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Average SG Putting\")\nplt.title(\"Average SG_P (Putting) by Year\")\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "unsupervised.html",
    "href": "unsupervised.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised Learning\n\n\nData Import\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npga = pd.read_csv(\"data/processed-data/pga_cleaned.csv\")\npga.head()\n\n\n\n\n\n\n\n\n\nname\nyear\ncountry\nscoring\ndrive_distance\ngir_pct\nsg_p\nsg_ttg\nsg_t\ntop_10\nwin\n\n\n\n\n0\nAaron Baddeley\n2007\nAUS\n70.088\n291.9\n60.35\n0.629\n0.435\n1.064\n7\n1\n\n\n1\nAaron Baddeley\n2008\nAUS\n70.196\n290.3\n62.02\n0.762\n0.123\n0.885\n3\n0\n\n\n2\nAaron Baddeley\n2009\nAUS\n71.153\n287.8\n59.57\n0.604\n-0.755\n-0.151\n2\n0\n\n\n3\nAaron Baddeley\n2010\nAUS\n70.995\n298.9\n64.60\n0.509\n-0.294\n0.208\n2\n0\n\n\n4\nAaron Baddeley\n2011\nAUS\n70.230\n296.2\n65.48\n0.324\n0.824\n1.148\n5\n1\n\n\n\n\n\n\n\n\n\nFilter Features\n\n\nCode\nfeatures = [\n    \"scoring\",\n    \"drive_distance\",\n    \"gir_pct\",\n    \"sg_p\",\n    \"sg_ttg\",\n    \"sg_t\",\n    \"top_10\",\n    \"win\"\n]\n\nX = pga[features].dropna()\nX.head()\n\n\n\n\n\n\n\n\n\nscoring\ndrive_distance\ngir_pct\nsg_p\nsg_ttg\nsg_t\ntop_10\nwin\n\n\n\n\n0\n70.088\n291.9\n60.35\n0.629\n0.435\n1.064\n7\n1\n\n\n1\n70.196\n290.3\n62.02\n0.762\n0.123\n0.885\n3\n0\n\n\n2\n71.153\n287.8\n59.57\n0.604\n-0.755\n-0.151\n2\n0\n\n\n3\n70.995\n298.9\n64.60\n0.509\n-0.294\n0.208\n2\n0\n\n\n4\n70.230\n296.2\n65.48\n0.324\n0.824\n1.148\n5\n1\n\n\n\n\n\n\n\n\n\nScale\n\n\nCode\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n\n\nCalculate optimal K mean\n\n\nCode\ninertia = []\nK_range = range(2, 10)\n\nfor k in K_range:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X_scaled)\n    inertia.append(km.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.plot(K_range, inertia, marker='o')\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Inertia (Within-Cluster SSE)\")\nplt.title(\"Elbow Method for Optimal k\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSet K mean as 4\n\n\nCode\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_scaled)\n\npga_clusters = pga.loc[X.index].copy()\npga_clusters[\"cluster\"] = clusters\n\n\n\n\nPCA graph\n\n\nCode\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\ncoords = pca.fit_transform(X_scaled)\n\npga_clusters[\"PC1\"] = coords[:,0]\npga_clusters[\"PC2\"] = coords[:,1]\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=pga_clusters,\n    x=\"PC1\", y=\"PC2\",\n    hue=\"cluster\", palette=\"tab10\", alpha=0.6\n)\nplt.title(\"PCA Visualization of Player Clusters\")\nplt.savefig(\"images/pca_clusters.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCluster Profile\n\n\nCode\ncluster_profile = pga_clusters.groupby(\"cluster\")[features].mean().round(2)\ncluster_profile\n\n\n\n\n\n\n\n\n\nscoring\ndrive_distance\ngir_pct\nsg_p\nsg_ttg\nsg_t\ntop_10\nwin\n\n\ncluster\n\n\n\n\n\n\n\n\n\n\n\n\n0\n70.86\n295.99\n67.46\n-0.18\n0.40\n0.22\n2.40\n0.06\n\n\n1\n71.69\n287.64\n63.65\n-0.10\n-0.54\n-0.63\n0.85\n0.05\n\n\n2\n69.91\n297.30\n67.45\n0.19\n0.96\n1.15\n6.75\n0.58\n\n\n3\n70.76\n287.14\n64.58\n0.29\n0.02\n0.31\n2.64\n0.10\n\n\n\n\n\n\n\n\n\nCluster player names\n\n\nCode\npga_clusters = pga.loc[X.index].copy()\npga_clusters[\"cluster\"] = clusters\n\n\ndef show_players(df, cluster_id, sort_col=\"scoring\", n=15):\n    return (\n        df[df[\"cluster\"] == cluster_id]\n        .sort_values(sort_col)\n        [[\"name\", \"year\", \"scoring\", \"sg_ttg\", \"sg_p\", \"top_10\", \"win\"]]\n        .head(n)\n    )\n\nshow_players(pga_clusters, 0, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 1, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 2, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 3, sort_col=\"sg_p\", n=20)\n\n\n\n\n\n\n\n\n\nname\nyear\nscoring\nsg_ttg\nsg_p\ntop_10\nwin\n\n\n\n\n1402\nJosÃ© Coceres\n2007\n70.737\n0.245\n-0.283\n3\n0\n\n\n2109\nRocco Mediate\n2008\n70.778\n0.287\n-0.264\n2\n0\n\n\n872\nFrancesco Molinari\n2019\n70.974\n0.250\n-0.211\n3\n0\n\n\n940\nGraeme McDowell\n2016\n70.899\n0.030\n-0.208\n5\n1\n\n\n203\nBilly Mayfair\n2012\n70.948\n0.128\n-0.205\n1\n0\n\n\n1641\nLuke Guthrie\n2013\n71.139\n0.219\n-0.205\n2\n0\n\n\n484\nCamilo Villegas\n2011\n71.024\n0.284\n-0.194\n4\n0\n\n\n2547\nTom Lehman\n2008\n70.791\n0.643\n-0.190\n2\n0\n\n\n858\nErnie Els\n2014\n70.736\n0.654\n-0.171\n3\n0\n\n\n2371\nSi Woo Kim\n2018\n71.367\n0.077\n-0.168\n5\n0\n\n\n2541\nTom Hoge\n2018\n70.945\n0.235\n-0.162\n3\n0\n\n\n678\nD.A. Points\n2013\n71.116\n0.160\n-0.161\n3\n1\n\n\n2224\nRyuji Imada\n2011\n71.115\n0.040\n-0.157\n2\n0\n\n\n2434\nSteven Alker\n2015\n71.094\n0.163\n-0.146\n0\n0\n\n\n614\nChris DiMarco\n2011\n70.909\n0.123\n-0.145\n0\n0\n\n\n884\nFreddie Jacobson\n2012\n70.561\n0.254\n-0.145\n1\n0\n\n\n761\nDavid Toms\n2012\n70.740\n0.331\n-0.141\n5\n0\n\n\n857\nErnie Els\n2013\n70.491\n0.444\n-0.136\n1\n0\n\n\n1909\nPadraig Harrington\n2013\n71.017\n0.042\n-0.131\n3\n0\n\n\n1716\nMartin Laird\n2013\n71.029\n0.136\n-0.127\n2\n1\n\n\n\n\n\n\n\n\n\nDisplay distinct three names for each clusters as an example\n\n\nCode\nfrom IPython.display import display\n\ndef pick_players(df, cluster_id, n=3):\n    cluster_df = (\n        df[df[\"cluster\"] == cluster_id]\n        .sort_values([\"year\", \"scoring\"])\n        [[\"name\", \"year\", \"scoring\", \"sg_ttg\", \"sg_p\", \"top_10\", \"win\"]]\n        .drop_duplicates(subset=[\"name\"])  \n    )\n    return cluster_df.head(n)\n\nfor c in range(4):\n    print(f\"\\n=== Cluster {c} ===\")\n    display(pick_players(pga_clusters, c, n=3))\n\n\n\n=== Cluster 0 ===\n\n\n\n\n\n\n\n\n\nname\nyear\nscoring\nsg_ttg\nsg_p\ntop_10\nwin\n\n\n\n\n114\nAnthony Kim\n2007\n70.128\n0.794\n-0.121\n4\n0\n\n\n1199\nJeff Overton\n2007\n70.339\n0.822\n0.060\n3\n0\n\n\n1489\nKenny Perry\n2007\n70.369\n0.981\n-0.337\n3\n0\n\n\n\n\n\n\n\n\n=== Cluster 1 ===\n\n\n\n\n\n\n\n\n\nname\nyear\nscoring\nsg_ttg\nsg_p\ntop_10\nwin\n\n\n\n\n78\nAnders Hansen\n2007\n70.856\n0.087\n-0.176\n0\n0\n\n\n930\nGlen Day\n2007\n71.172\n-0.238\n0.063\n0\n0\n\n\n906\nGavin Coles\n2007\n71.245\n-0.626\n0.340\n2\n0\n\n\n\n\n\n\n\n\n=== Cluster 2 ===\n\n\n\n\n\n\n\n\n\nname\nyear\nscoring\nsg_ttg\nsg_p\ntop_10\nwin\n\n\n\n\n2500\nTiger Woods\n2007\n67.794\n2.380\n0.712\n12\n7\n\n\n851\nErnie Els\n2007\n69.294\n1.609\n-0.031\n5\n0\n\n\n1421\nJustin Rose\n2007\n69.301\n0.816\n0.061\n7\n0\n\n\n\n\n\n\n\n\n=== Cluster 3 ===\n\n\n\n\n\n\n\n\n\nname\nyear\nscoring\nsg_ttg\nsg_p\ntop_10\nwin\n\n\n\n\n123\nArron Oberholser\n2007\n69.807\n0.821\n0.362\n5\n0\n\n\n1903\nPadraig Harrington\n2007\n69.822\n0.666\n0.260\n3\n1\n\n\n2505\nTim Clark\n2007\n69.877\n0.293\n0.695\n6\n0"
  },
  {
    "objectID": "data_collection/main.html",
    "href": "data_collection/main.html",
    "title": "Data Collection from PGA Website",
    "section": "",
    "text": "This file is used to scrape data from PGA official website\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re\n\n\n\n\nCode\nHEADERS = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n                  \"Chrome/120.0.0.0 Safari/537.36\"\n}\n\ndef scrape_espn_player_stats(season: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Scrape ESPN 'PGA TOUR Player Stats {season}' page.\n\n    Returns a DataFrame with:\n    NAME, Season, EARNINGS, CUP, EVNTS, RNDS, CUTS, TOP10, WINS,\n    SCORE, DDIS, DACC, GIR, PUTTS, SAND, BIRDS\n    \"\"\"\n    url = f\"https://www.espn.com/golf/stats/player/_/season/{season}\"\n    print(f\"Scraping {season} from {url}\")\n\n    resp = requests.get(url, headers=HEADERS)\n    resp.raise_for_status()\n\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n\n    # ---------- 1) Get raw text lines ----------\n    text = soup.get_text(\"\\n\")\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n\n    # This header line appears right before the stat rows.\n    # In the flattened text it looks like this (no spaces):\n    # EARNINGSCUPEVNTSRNDSCUTSTOP10WINSSCOREDDISDACCGIRPUTTSSANDBIRDS\n    header_compact = \"EARNINGSCUPEVNTSRNDSCUTSTOP10WINSSCOREDDISDACCGIRPUTTSSANDBIRDS\"\n\n    stats_start = None\n    for i, ln in enumerate(lines):\n        if ln.replace(\" \", \"\") == header_compact:\n            stats_start = i + 1  # stats start on next line\n            break\n\n    if stats_start is None:\n        raise ValueError(f\"Could not find stats header for season {season}.\")\n\n    # ---------- 2) Collect stats lines ----------\n    stat_lines = []\n    for ln in lines[stats_start:]:\n        if ln.startswith(\"Show More\"):\n            break  # end of table\n        if not ln.startswith(\"$\"):\n            continue  # skip non-stat lines\n        tokens = ln.split()\n        # Should be: $EARNINGS CUP EVNTS RNDS CUTS TOP10 WINS SCORE DDIS DACC GIR PUTTS SAND BIRDS\n        if len(tokens) != 14:\n            continue\n        stat_lines.append(tokens)\n\n    if not stat_lines:\n        raise ValueError(f\"No stat rows found for season {season}.\")\n\n    # ---------- 3) Collect player names ----------\n    # Player links have URLs like /golf/player/_/id/3599/brian-stuard\n    player_links = soup.find_all(\"a\", href=re.compile(r\"/golf/player/_/id/\"))\n    names = []\n    for a in player_links:\n        name = a.get_text(strip=True)\n        if name and name not in names:\n            names.append(name)\n\n    # We only have stats for the first N players; align lengths just in case\n    n = min(len(names), len(stat_lines))\n    names = names[:n]\n    stat_lines = stat_lines[:n]\n\n    # ---------- 4) Build DataFrame ----------\n    stat_cols = [\n        \"EARNINGS\", \"CUP\", \"EVNTS\", \"RNDS\", \"CUTS\", \"TOP10\", \"WINS\",\n        \"SCORE\", \"DDIS\", \"DACC\", \"GIR\", \"PUTTS\", \"SAND\", \"BIRDS\"\n    ]\n\n    records = []\n    for name, row in zip(names, stat_lines):\n        rec = {\n            \"NAME\": name,\n            \"Season\": season,\n        }\n        rec[\"EARNINGS\"] = row[0]    # '$x,xxx'\n        # map remaining 13 stats\n        for col, val in zip(stat_cols[1:], row[1:]):\n            rec[col] = val\n        records.append(rec)\n\n    df = pd.DataFrame.from_records(records)\n    return df\n\n\n\n\n\nCode\ndef scrape_espn_range(start_season: int, end_season: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Scrape multiple seasons from ESPN and stack into one DataFrame.\n    \"\"\"\n    all_frames = []\n    for season in range(start_season, end_season + 1):\n        try:\n            df_season = scrape_espn_player_stats(season)\n            all_frames.append(df_season)\n        except Exception as e:\n            print(f\"Season {season} failed: {e}\")\n\n    if not all_frames:\n        raise RuntimeError(\"No seasons were successfully scraped.\")\n\n    combined = pd.concat(all_frames, ignore_index=True)\n    return combined\n\n\n\n\nCode\nespn_recent = scrape_espn_range(2019, 2025)\n\n# Save to your project raw-data folder\nespn_recent.to_csv(\"data/raw-data/espn_pga_stats_2019_2025_top_players.csv\", index=False)\n\nespn_recent.head()\n\n\nScraping 2019 from https://www.espn.com/golf/stats/player/_/season/2019\nSeason 2019 failed: Could not find stats header for season 2019.\nScraping 2020 from https://www.espn.com/golf/stats/player/_/season/2020\nSeason 2020 failed: Could not find stats header for season 2020.\nScraping 2021 from https://www.espn.com/golf/stats/player/_/season/2021\nSeason 2021 failed: Could not find stats header for season 2021.\nScraping 2022 from https://www.espn.com/golf/stats/player/_/season/2022\nSeason 2022 failed: Could not find stats header for season 2022.\nScraping 2023 from https://www.espn.com/golf/stats/player/_/season/2023\nSeason 2023 failed: Could not find stats header for season 2023.\nScraping 2024 from https://www.espn.com/golf/stats/player/_/season/2024\nSeason 2024 failed: Could not find stats header for season 2024.\nScraping 2025 from https://www.espn.com/golf/stats/player/_/season/2025\nSeason 2025 failed: Could not find stats header for season 2025.\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[31], line 1\n----&gt; 1 espn_recent = scrape_espn_range(2019, 2025)\n      3 # Save to your project raw-data folder\n      4 espn_recent.to_csv(\"data/raw-data/espn_pga_stats_2019_2025_top_players.csv\", index=False)\n\nCell In[30], line 14, in scrape_espn_range(start_season, end_season)\n     11         print(f\"Season {season} failed: {e}\")\n     13 if not all_frames:\n---&gt; 14     raise RuntimeError(\"No seasons were successfully scraped.\")\n     16 combined = pd.concat(all_frames, ignore_index=True)\n     17 return combined\n\nRuntimeError: No seasons were successfully scraped."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "Over the last 15–20 years, professional golf has quietly become a data-driven sport. Launch monitors, performance tracking apps, and shot-level databases have transformed how coaches and players think about improvement. Decisions that used to rely on feel and memory are now increasingly guided by numbers.\nOne of the most important developments in this shift is the family of strokes gained (SG) metrics, which measure how many strokes a player gains or loses on each shot relative to the PGA Tour field. Mark Broadie’s work formally introduced this framework and showed that differences in the long game (driving and approach play) explain most of the variation in scoring among PGA Tour players, with putting playing a smaller—though still meaningful—role.\nAt the same time, official reports from the USGA and R&A document a steady increase in driving distance on professional tours since the early 2000s, raising questions about how distance, strategy, and course set-up interact in the modern game.\nThis project builds on that context by using publicly available PGA Tour summary datasets (2007–2022) to ask:\n\nHow have key performance metrics such as driving distance and strokes gained evolved over time?\nWhich metrics are most strongly associated with lower scoring?\nWhat separates winners from the rest of the field in the modern PGA Tour environment?\n\nBy combining data cleaning, exploratory analysis, unsupervised learning (player clustering), and supervised models (scoring and win prediction), the project aims to connect high-level trends in the sport with practical performance insights for coaches, analysts, and competitive players.\n\n\n\nThis analysis is organized around six core questions:\n\nHow have key PGA Tour metrics evolved from 2007–2022?\n\nTrends in scoring average, driving distance, GIR%, and strokes gained metrics.\n\nAre there identifiable “types” of PGA Tour players?\n\nUse unsupervised learning (k-means + PCA) to uncover clusters such as elite ball-strikers, putting specialists, and tour-average grinders.\n\nWhich performance metrics best predict scoring average?\n\nCompare the predictive power of SG Tee-to-Green, SG Putting, GIR%, and driving distance using regression models.\n\nWhat distinguishes winning players from the rest of the field?\n\nModel the probability of earning at least one win in a season from SG metrics, GIR%, and distance.\n\nIs modern success driven more by strokes gained tee-to-green or putting?\n\nEvaluate effect sizes and feature importance from both scoring and win models, and compare them with prior work that emphasizes the long game.\n\nDoes added distance directly translate into better scores or more wins when compared to strokes gained metrics?\n\nContrast the role of raw distance with more context-aware measures like SG Tee-to-Green, which capture not just how far a player hits it, but how efficiently those shots lower score.\n\n\nThe rest of the site is structured to walk through this pipeline:\n\nData Collection – Documenting and citing the two GitHub datasets used and how they map to PGA Tour concepts.\n\nData Cleaning & Integration – Reconciling column names, fixing numeric formats, standardizing player names, and merging seasons into a single panel.\n\nExploratory Data Analysis (EDA) – Visualizing trends and relationships that speak directly to Research Question 1.\n\nUnsupervised Learning – Discovering player archetypes (Question 2).\n\nSupervised Learning – Predicting scoring and wins to address Questions 3–6.\n\nConclusion – Summarizing key insights, limitations, and possible directions for future work.\n\n\n\n\nBroadie, M. (2011). Assessing Golfer Performance Using Golfmetrics. Interfaces, 41(1), 17–26.\nhttps://pubsonline.informs.org/doi/abs/10.1287/inte.1100.0532\nBroadie, M. (2014). Assessing Golfer Performance on the PGA Tour.\nhttps://faculty.gsb.columbia.edu/michael-broadie/wp-content/uploads/2015/07/QuantifyingGolfersResearch.pdf\nUnited States Golf Association & The R&A. (2022). Distance Insights Report – Annual Driving Distance and Clubhead Speed Report.\nhttps://www.usga.org/content/usga/home-page/advancing-the-game/distance-insights/2022-distance-insights.html"
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Landing page",
    "section": "",
    "text": "Over the last 15–20 years, professional golf has quietly become a data-driven sport. Launch monitors, performance tracking apps, and shot-level databases have transformed how coaches and players think about improvement. Decisions that used to rely on feel and memory are now increasingly guided by numbers.\nOne of the most important developments in this shift is the family of strokes gained (SG) metrics, which measure how many strokes a player gains or loses on each shot relative to the PGA Tour field. Mark Broadie’s work formally introduced this framework and showed that differences in the long game (driving and approach play) explain most of the variation in scoring among PGA Tour players, with putting playing a smaller—though still meaningful—role.\nAt the same time, official reports from the USGA and R&A document a steady increase in driving distance on professional tours since the early 2000s, raising questions about how distance, strategy, and course set-up interact in the modern game.\nThis project builds on that context by using publicly available PGA Tour summary datasets (2007–2022) to ask:\n\nHow have key performance metrics such as driving distance and strokes gained evolved over time?\nWhich metrics are most strongly associated with lower scoring?\nWhat separates winners from the rest of the field in the modern PGA Tour environment?\n\nBy combining data cleaning, exploratory analysis, unsupervised learning (player clustering), and supervised models (scoring and win prediction), the project aims to connect high-level trends in the sport with practical performance insights for coaches, analysts, and competitive players."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Landing page",
    "section": "",
    "text": "This analysis is organized around six core questions:\n\nHow have key PGA Tour metrics evolved from 2007–2022?\n\nTrends in scoring average, driving distance, GIR%, and strokes gained metrics.\n\nAre there identifiable “types” of PGA Tour players?\n\nUse unsupervised learning (k-means + PCA) to uncover clusters such as elite ball-strikers, putting specialists, and tour-average grinders.\n\nWhich performance metrics best predict scoring average?\n\nCompare the predictive power of SG Tee-to-Green, SG Putting, GIR%, and driving distance using regression models.\n\nWhat distinguishes winning players from the rest of the field?\n\nModel the probability of earning at least one win in a season from SG metrics, GIR%, and distance.\n\nIs modern success driven more by strokes gained tee-to-green or putting?\n\nEvaluate effect sizes and feature importance from both scoring and win models, and compare them with prior work that emphasizes the long game.\n\nDoes added distance directly translate into better scores or more wins when compared to strokes gained metrics?\n\nContrast the role of raw distance with more context-aware measures like SG Tee-to-Green, which capture not just how far a player hits it, but how efficiently those shots lower score.\n\n\nThe rest of the site is structured to walk through this pipeline:\n\nData Collection – Documenting and citing the two GitHub datasets used and how they map to PGA Tour concepts.\n\nData Cleaning & Integration – Reconciling column names, fixing numeric formats, standardizing player names, and merging seasons into a single panel.\n\nExploratory Data Analysis (EDA) – Visualizing trends and relationships that speak directly to Research Question 1.\n\nUnsupervised Learning – Discovering player archetypes (Question 2).\n\nSupervised Learning – Predicting scoring and wins to address Questions 3–6.\n\nConclusion – Summarizing key insights, limitations, and possible directions for future work."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Landing page",
    "section": "",
    "text": "Broadie, M. (2011). Assessing Golfer Performance Using Golfmetrics. Interfaces, 41(1), 17–26.\nhttps://pubsonline.informs.org/doi/abs/10.1287/inte.1100.0532\nBroadie, M. (2014). Assessing Golfer Performance on the PGA Tour.\nhttps://faculty.gsb.columbia.edu/michael-broadie/wp-content/uploads/2015/07/QuantifyingGolfersResearch.pdf\nUnited States Golf Association & The R&A. (2022). Distance Insights Report – Annual Driving Distance and Clubhead Speed Report.\nhttps://www.usga.org/content/usga/home-page/advancing-the-game/distance-insights/2022-distance-insights.html"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Load Data: This step establishes the two primary data sources that will later be merged into a unified dataset for analysis.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\nraw_path = \"data/raw-data/pgatour_raw.csv\"\nfull_path = \"data/raw-data/pga_full.csv\"\n\ndf_raw = pd.read_csv(raw_path, encoding=\"latin1\")\ndf_full = pd.read_csv(full_path, encoding=\"latin1\")"
  },
  {
    "objectID": "data_cleaning.html#save-merged-data",
    "href": "data_cleaning.html#save-merged-data",
    "title": "Data Cleaning",
    "section": "Save Merged Data",
    "text": "Save Merged Data\n\n\nCode\noutput_path = \"data/processed-data/pga_cleaned.csv\"\ncombined.to_csv(output_path, index=False)\noutput_path\n\n\n'data/processed-data/pga_cleaned.csv'"
  },
  {
    "objectID": "data_cleaning.html#before-after-comparison",
    "href": "data_cleaning.html#before-after-comparison",
    "title": "Data Cleaning",
    "section": "Before & After Comparison",
    "text": "Before & After Comparison\nConfirm the transformation\n\n\nCode\ndf_raw.head()\ndf_full.head()\ncombined.head()\n\n\n\n\n\n\n\n\n\nname\nyear\ncountry\nscoring\ndrive_distance\ngir_pct\nsg_p\nsg_ttg\nsg_t\ntop_10\nwin\n\n\n\n\n0\nAaron Baddeley\n2007\nAUS\n70.088\n291.9\n60.35\n0.629\n0.435\n1.064\n7\n1\n\n\n195\nAaron Baddeley\n2008\nAUS\n70.196\n290.3\n62.02\n0.762\n0.123\n0.885\n3\n0\n\n\n389\nAaron Baddeley\n2009\nAUS\n71.153\n287.8\n59.57\n0.604\n-0.755\n-0.151\n2\n0\n\n\n571\nAaron Baddeley\n2010\nAUS\n70.995\n298.9\n64.60\n0.509\n-0.294\n0.208\n2\n0\n\n\n760\nAaron Baddeley\n2011\nAUS\n70.230\n296.2\n65.48\n0.324\n0.824\n1.148\n5\n1"
  },
  {
    "objectID": "data_collection.html",
    "href": "data_collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "This project analyzes PGA Tour player performance from 2007–2022.\nInstead of scraping data from PGA live websites, I use two publicly available raw CSV files on GitHub to apply for my analysis.\nThese files are treated as raw data and stored in:\ndata/ raw-data/ pgatour_raw.csv pga_full.csv"
  },
  {
    "objectID": "data_collection.html#overview",
    "href": "data_collection.html#overview",
    "title": "Data Collection",
    "section": "",
    "text": "This project analyzes PGA Tour player performance from 2007–2022.\nInstead of scraping data from PGA live websites, I use two publicly available raw CSV files on GitHub to apply for my analysis.\nThese files are treated as raw data and stored in:\ndata/ raw-data/ pgatour_raw.csv pga_full.csv"
  }
]