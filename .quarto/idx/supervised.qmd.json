{"title":"Supervised Learning","markdown":{"yaml":{"title":"Supervised Learning","format":{"html":{"toc":true,"code-fold":true,"embed-resources":true}},"editor":"visual"},"headingText":"Setup","containsRefs":false,"markdown":"\n\nIn this section, I use supervised learning to answer four main research questions:\n\n> -   Which performance metrics best predict scoring average on the PGA Tour?\n>\n> -   What distinguishes winning players from the rest of the field?\n>\n> -   Is modern success driven more by strokes-gained tee-to-green or putting?\n>\n> -   Does distance directly contribute to better score or titles won when compared to strokes-gain metrics?\n\nI fit both regression models (for scoring average) and classification models (for whether a player wins at least one event in a season) and compare their performance and feature importance.\n\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import root_mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n    accuracy_score,\n    roc_auc_score,\n    classification_report,\n)\n```\n\n### Load data\n\n```{python}\npga = pd.read_csv(\"data/processed-data/pga_cleaned.csv\")\npga.head()\npga.columns\n```\n\n### Regression (Predict Score)\n\nThe first supervised task is predicting scoring average using only a few core performance metrics:\n\n-   **Driving Distance**\n\n-   **Greens in Regulation % (GIR%)**\n\n-   **Strokes-Gained Putting (SG_P)**\n\n-   **Strokes-Gained Tee-to-Green (SG_TTG)**\n\nThese four features represent the main components of performance: distance, approach/accuracy, putting, and long-game quality.\n\nVariables defined:\n\n-   `X_reg` = the matrix of these four predictors\n\n-   `y_reg` = the target variable `scoring`\n\nRows with missing values were dropped to ensure the regression model has complete data for all inputs.\n\n```{python}\nreg_features = [\n  \"drive_distance\",\n  \"gir_pct\",\n  \"sg_p\",\n  \"sg_ttg\",\n]\nreg_target = \"scoring\"\n\nreg_df = pga[reg_features + [reg_target]].dropna()\nX_reg = reg_df[reg_features]\ny_reg = reg_df[reg_target]\n\nX_reg.shape, y_reg.shape\n```\n\n### Training and testing split (Predict Score)\n\nTo properly evaluate the regression model, I split the data into:\n\n-   **80% training data** – used to fit the model\n\n-   **20% testing data** – held out for evaluation\n\nI also standardize the predictors (mean 0, variance 1) using StandardScaler. This is especially important for Linear Regression, which can be sensitive to feature scales, and makes coefficients more interpretable in standardized units.\n\n```{python}\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\nX_reg, y_reg, test_size=0.2, random_state=42\n)\n\nscaler_reg = StandardScaler()\nX_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\nX_test_reg_scaled = scaler_reg.transform(X_test_reg)\n```\n\n### Linear Regression (Predict Score)\n\nIn this step, I fit a linear regression model on the scaled training data and evaluate it on the test data.\n\n-   **RMSE** (root mean squared error) measures the typical prediction error in strokes.\n\n-   **R²** measures how much of the variance in scoring average is explained by the four predictors.\n\nThe results:\n\n-   RMSE ≈ 0.19\n\n-   R² ≈ 0.91\n\nThis means the model explains about **91%** of the variation in scoring averages. That is extremely strong for sports performance data, suggesting that these four metrics capture most of what drives scoring differences between players.\n\n```{python}\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_reg_scaled, y_train_reg)\n\ny_pred_lin = lin_reg.predict(X_test_reg_scaled)\n\nrmse_lin = root_mean_squared_error(y_test_reg, y_pred_lin)\nr2_lin = r2_score(y_test_reg, y_pred_lin)\n\nprint(\"Linear Regression (Predict Score)\")\nprint(\" RMSE:\", rmse_lin)\nprint(\" R^2 :\", r2_lin)\n```\n\n### Random Forest (Predict Score)\n\nUnlike linear regression, a random forest can automatically capture nonlinearities and interactions between variables.\n\nThe results:\n\n-   RMSE ≈ 0.21\n\n-   R² ≈ 0.90\n\nThis shows that:\n\n-   The relationship between scoring and these features is mostly linear, since the simpler linear model actually performs slightly better.\n\n-   Random Forest is still useful for its feature importance output, which helps quantify which predictors matter most.\n\n```{python}\nrf_reg = RandomForestRegressor(\n  n_estimators=300,\n  random_state=42,\n  max_depth=None,\n  n_jobs=-1\n)\nrf_reg.fit(X_train_reg, y_train_reg)\n\ny_pred_rf = rf_reg.predict(X_test_reg)\n\nrmse_rf = root_mean_squared_error(y_test_reg, y_pred_rf)\nr2_rf = r2_score(y_test_reg, y_pred_rf)\n\nprint(\"Random Forest Regression (Predict Score)\")\nprint(\"  RMSE:\", rmse_rf)\nprint(\"  R^2 :\", r2_rf)\n```\n\n### Feature Importance (Predict Score)\n\nHere, I extract feature importance values from the random forest regression model and visualize them as a bar chart.\n\nThe ranking:\n\n1.  **SG_TTG**\n\n2.  **SG_P**\n\n3.  **GIR%**\n\n4.  **Driving Distance**\n\n**Interpretation:**\n\n-   Strokes Gained Tee-to-Green dominates scoring prediction.\n\n-   Putting still matters, but contributes less than overall long-game quality.\n\n-   Distance is the least important of the four, once strokes gained metrics are included.\n\nThis matches both the EDA and existing literature: scoring is primarily driven by tee-to-green performance, not just hitting it far.\n\n```{python}\nimportances_reg = pd.Series(\nrf_reg.feature_importances_,\nindex=reg_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_reg.values, y=importances_reg.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Score)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_scoring.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_reg\n```\n\n### Classification Model (Predict Win)\n\nNow I switch from predicting a continuous outcome (scoring) to a binary outcome: whether a player wins at least one event in a season.\n\nI create:\n\n-   `has_win = 1` if `win > 0`\n\n-   `has_win = 0` otherwise\n\nThe same four features are used:\n\n-   **Driving Distance**\n\n-   **Greens in Regulation % (GIR%)**\n\n-   **Strokes-Gained Putting (SG_P)**\n\n-   **Strokes-Gained Tee-to-Green (SG_TTG)**\n\n```{python}\n\npga[\"has_win\"] = (pga[\"win\"] > 0).astype(int)\n\nclf_features = [\n\"drive_distance\",\n\"gir_pct\",\n\"sg_p\",\n\"sg_ttg\",\n]\n\nclf_df = pga[clf_features + [\"has_win\"]].dropna()\nX_clf = clf_df[clf_features]\ny_clf = clf_df[\"has_win\"]\n\nX_clf.shape, y_clf.value_counts()\n```\n\n### Training and testing split (Predict Win)\n\nSimilar to the regression case, I:\n\n-   Split into training (80%) and test (20%) sets\n\n-   Use `stratify=y_clf` so the class balance (winner vs non-winner) is preserved\n\n-   Standardize predictors for logistic regression\n\nThis setup supports fair model evaluation, especially important because only a small percentage of seasons include a win, which makes the outcome imbalanced.\n\n```{python}\n\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\nX_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n)\n\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n```\n\n### Logistic Regression (Predict Win)\n\nI then fit a Logistic Regression model on the scaled training data and evaluate it on the test set.\n\nMetrics:\n\n-   **Accuracy** – overall percent of correct predictions\n\n-   **ROC AUC** – how well the model ranks winners above non-winners\n\n-   **classification_report** – precision/recall/F1 for winners vs non-winners\n\nThe accuracy is high (\\~0.88), but recall for the winning class is low. This reflects reality: winning is rare and noisy, and many seasons with strong stats still result in no wins.\n\n```{python}\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train_clf_scaled, y_train_clf)\n\ny_pred_log = log_reg.predict(X_test_clf_scaled)\ny_prob_log = log_reg.predict_proba(X_test_clf_scaled)[:, 1]\n\nacc_log = accuracy_score(y_test_clf, y_pred_log)\nauc_log = roc_auc_score(y_test_clf, y_prob_log)\n\nprint(\"Logistic Regression (Predict Win)\")\nprint(\"  Accuracy:\", acc_log)\nprint(\"  ROC AUC :\", auc_log)\nprint(classification_report(y_test_clf, y_pred_log))\n```\n\n### Random Forest (Predict Win)\n\nI also train a Random Forest Classifier with 400 trees. Again, this model is less sensitive to scaling and can capture non-linear decision boundaries.\n\nEvaluation shows:\n\n-   Accuracy around 0.88\n\n-   ROC AUC around 0.71\n\n-   Improved but still modest recall for winners\n\nThis confirms that winning is harder to predict than scoring. Tournament victories depend on many factors not captured in season aggregates (course fit, weather, clutch performance, luck, etc.).\n\n```{python}\nrf_clf = RandomForestClassifier(\nn_estimators=400,\nrandom_state=42,\nmax_depth=None,\nn_jobs=-1\n)\nrf_clf.fit(X_train_clf, y_train_clf)\n\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_prob_rf_clf = rf_clf.predict_proba(X_test_clf)[:, 1]\n\nacc_rf = accuracy_score(y_test_clf, y_pred_rf_clf)\nauc_rf = roc_auc_score(y_test_clf, y_prob_rf_clf)\n\nprint(\"Random Forest Classifier (Predict Win)\")\nprint(\"  Accuracy:\", acc_rf)\nprint(\"  ROC AUC :\", auc_rf)\nprint(classification_report(y_test_clf, y_pred_rf_clf))\n```\n\n### Feature Importance (Predict Win)\n\nFinally, I extract feature importance from the random forest classifier and visualize it.\n\nThe ranking tends to be:\n\n-   SG_TTG – still the top predictor\n\n-   GIR%, Driving Distance, and SG_P contribute at a similar level\n\n**Interpretation:**\n\n-   Winning a tournament requires a complete game: long-game strength, solid GIR%, putting, and distance all matter.\n\n-   SG_TTG remains foundational, but putting and distance play a more visible role in differentiating players at the very top.\n\nOverall, the supervised learning section shows:\n\n-   Scoring average is highly explainable and strongly driven by tee-to-green performance.\n\n-   Winning is much harder to predict, but still leans heavily on long-game quality, with meaningful contributions from putting and distance.\n\n```{python}\nimportances_clf = pd.Series(\nrf_clf.feature_importances_,\nindex=clf_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_clf.values, y=importances_clf.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Win)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_wins.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_clf\n```\n","srcMarkdownNoYaml":"\n\nIn this section, I use supervised learning to answer four main research questions:\n\n> -   Which performance metrics best predict scoring average on the PGA Tour?\n>\n> -   What distinguishes winning players from the rest of the field?\n>\n> -   Is modern success driven more by strokes-gained tee-to-green or putting?\n>\n> -   Does distance directly contribute to better score or titles won when compared to strokes-gain metrics?\n\nI fit both regression models (for scoring average) and classification models (for whether a player wins at least one event in a season) and compare their performance and feature importance.\n\n### Setup\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import root_mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n    accuracy_score,\n    roc_auc_score,\n    classification_report,\n)\n```\n\n### Load data\n\n```{python}\npga = pd.read_csv(\"data/processed-data/pga_cleaned.csv\")\npga.head()\npga.columns\n```\n\n### Regression (Predict Score)\n\nThe first supervised task is predicting scoring average using only a few core performance metrics:\n\n-   **Driving Distance**\n\n-   **Greens in Regulation % (GIR%)**\n\n-   **Strokes-Gained Putting (SG_P)**\n\n-   **Strokes-Gained Tee-to-Green (SG_TTG)**\n\nThese four features represent the main components of performance: distance, approach/accuracy, putting, and long-game quality.\n\nVariables defined:\n\n-   `X_reg` = the matrix of these four predictors\n\n-   `y_reg` = the target variable `scoring`\n\nRows with missing values were dropped to ensure the regression model has complete data for all inputs.\n\n```{python}\nreg_features = [\n  \"drive_distance\",\n  \"gir_pct\",\n  \"sg_p\",\n  \"sg_ttg\",\n]\nreg_target = \"scoring\"\n\nreg_df = pga[reg_features + [reg_target]].dropna()\nX_reg = reg_df[reg_features]\ny_reg = reg_df[reg_target]\n\nX_reg.shape, y_reg.shape\n```\n\n### Training and testing split (Predict Score)\n\nTo properly evaluate the regression model, I split the data into:\n\n-   **80% training data** – used to fit the model\n\n-   **20% testing data** – held out for evaluation\n\nI also standardize the predictors (mean 0, variance 1) using StandardScaler. This is especially important for Linear Regression, which can be sensitive to feature scales, and makes coefficients more interpretable in standardized units.\n\n```{python}\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\nX_reg, y_reg, test_size=0.2, random_state=42\n)\n\nscaler_reg = StandardScaler()\nX_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\nX_test_reg_scaled = scaler_reg.transform(X_test_reg)\n```\n\n### Linear Regression (Predict Score)\n\nIn this step, I fit a linear regression model on the scaled training data and evaluate it on the test data.\n\n-   **RMSE** (root mean squared error) measures the typical prediction error in strokes.\n\n-   **R²** measures how much of the variance in scoring average is explained by the four predictors.\n\nThe results:\n\n-   RMSE ≈ 0.19\n\n-   R² ≈ 0.91\n\nThis means the model explains about **91%** of the variation in scoring averages. That is extremely strong for sports performance data, suggesting that these four metrics capture most of what drives scoring differences between players.\n\n```{python}\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_reg_scaled, y_train_reg)\n\ny_pred_lin = lin_reg.predict(X_test_reg_scaled)\n\nrmse_lin = root_mean_squared_error(y_test_reg, y_pred_lin)\nr2_lin = r2_score(y_test_reg, y_pred_lin)\n\nprint(\"Linear Regression (Predict Score)\")\nprint(\" RMSE:\", rmse_lin)\nprint(\" R^2 :\", r2_lin)\n```\n\n### Random Forest (Predict Score)\n\nUnlike linear regression, a random forest can automatically capture nonlinearities and interactions between variables.\n\nThe results:\n\n-   RMSE ≈ 0.21\n\n-   R² ≈ 0.90\n\nThis shows that:\n\n-   The relationship between scoring and these features is mostly linear, since the simpler linear model actually performs slightly better.\n\n-   Random Forest is still useful for its feature importance output, which helps quantify which predictors matter most.\n\n```{python}\nrf_reg = RandomForestRegressor(\n  n_estimators=300,\n  random_state=42,\n  max_depth=None,\n  n_jobs=-1\n)\nrf_reg.fit(X_train_reg, y_train_reg)\n\ny_pred_rf = rf_reg.predict(X_test_reg)\n\nrmse_rf = root_mean_squared_error(y_test_reg, y_pred_rf)\nr2_rf = r2_score(y_test_reg, y_pred_rf)\n\nprint(\"Random Forest Regression (Predict Score)\")\nprint(\"  RMSE:\", rmse_rf)\nprint(\"  R^2 :\", r2_rf)\n```\n\n### Feature Importance (Predict Score)\n\nHere, I extract feature importance values from the random forest regression model and visualize them as a bar chart.\n\nThe ranking:\n\n1.  **SG_TTG**\n\n2.  **SG_P**\n\n3.  **GIR%**\n\n4.  **Driving Distance**\n\n**Interpretation:**\n\n-   Strokes Gained Tee-to-Green dominates scoring prediction.\n\n-   Putting still matters, but contributes less than overall long-game quality.\n\n-   Distance is the least important of the four, once strokes gained metrics are included.\n\nThis matches both the EDA and existing literature: scoring is primarily driven by tee-to-green performance, not just hitting it far.\n\n```{python}\nimportances_reg = pd.Series(\nrf_reg.feature_importances_,\nindex=reg_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_reg.values, y=importances_reg.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Score)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_scoring.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_reg\n```\n\n### Classification Model (Predict Win)\n\nNow I switch from predicting a continuous outcome (scoring) to a binary outcome: whether a player wins at least one event in a season.\n\nI create:\n\n-   `has_win = 1` if `win > 0`\n\n-   `has_win = 0` otherwise\n\nThe same four features are used:\n\n-   **Driving Distance**\n\n-   **Greens in Regulation % (GIR%)**\n\n-   **Strokes-Gained Putting (SG_P)**\n\n-   **Strokes-Gained Tee-to-Green (SG_TTG)**\n\n```{python}\n\npga[\"has_win\"] = (pga[\"win\"] > 0).astype(int)\n\nclf_features = [\n\"drive_distance\",\n\"gir_pct\",\n\"sg_p\",\n\"sg_ttg\",\n]\n\nclf_df = pga[clf_features + [\"has_win\"]].dropna()\nX_clf = clf_df[clf_features]\ny_clf = clf_df[\"has_win\"]\n\nX_clf.shape, y_clf.value_counts()\n```\n\n### Training and testing split (Predict Win)\n\nSimilar to the regression case, I:\n\n-   Split into training (80%) and test (20%) sets\n\n-   Use `stratify=y_clf` so the class balance (winner vs non-winner) is preserved\n\n-   Standardize predictors for logistic regression\n\nThis setup supports fair model evaluation, especially important because only a small percentage of seasons include a win, which makes the outcome imbalanced.\n\n```{python}\n\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\nX_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n)\n\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n```\n\n### Logistic Regression (Predict Win)\n\nI then fit a Logistic Regression model on the scaled training data and evaluate it on the test set.\n\nMetrics:\n\n-   **Accuracy** – overall percent of correct predictions\n\n-   **ROC AUC** – how well the model ranks winners above non-winners\n\n-   **classification_report** – precision/recall/F1 for winners vs non-winners\n\nThe accuracy is high (\\~0.88), but recall for the winning class is low. This reflects reality: winning is rare and noisy, and many seasons with strong stats still result in no wins.\n\n```{python}\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train_clf_scaled, y_train_clf)\n\ny_pred_log = log_reg.predict(X_test_clf_scaled)\ny_prob_log = log_reg.predict_proba(X_test_clf_scaled)[:, 1]\n\nacc_log = accuracy_score(y_test_clf, y_pred_log)\nauc_log = roc_auc_score(y_test_clf, y_prob_log)\n\nprint(\"Logistic Regression (Predict Win)\")\nprint(\"  Accuracy:\", acc_log)\nprint(\"  ROC AUC :\", auc_log)\nprint(classification_report(y_test_clf, y_pred_log))\n```\n\n### Random Forest (Predict Win)\n\nI also train a Random Forest Classifier with 400 trees. Again, this model is less sensitive to scaling and can capture non-linear decision boundaries.\n\nEvaluation shows:\n\n-   Accuracy around 0.88\n\n-   ROC AUC around 0.71\n\n-   Improved but still modest recall for winners\n\nThis confirms that winning is harder to predict than scoring. Tournament victories depend on many factors not captured in season aggregates (course fit, weather, clutch performance, luck, etc.).\n\n```{python}\nrf_clf = RandomForestClassifier(\nn_estimators=400,\nrandom_state=42,\nmax_depth=None,\nn_jobs=-1\n)\nrf_clf.fit(X_train_clf, y_train_clf)\n\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_prob_rf_clf = rf_clf.predict_proba(X_test_clf)[:, 1]\n\nacc_rf = accuracy_score(y_test_clf, y_pred_rf_clf)\nauc_rf = roc_auc_score(y_test_clf, y_prob_rf_clf)\n\nprint(\"Random Forest Classifier (Predict Win)\")\nprint(\"  Accuracy:\", acc_rf)\nprint(\"  ROC AUC :\", auc_rf)\nprint(classification_report(y_test_clf, y_pred_rf_clf))\n```\n\n### Feature Importance (Predict Win)\n\nFinally, I extract feature importance from the random forest classifier and visualize it.\n\nThe ranking tends to be:\n\n-   SG_TTG – still the top predictor\n\n-   GIR%, Driving Distance, and SG_P contribute at a similar level\n\n**Interpretation:**\n\n-   Winning a tournament requires a complete game: long-game strength, solid GIR%, putting, and distance all matter.\n\n-   SG_TTG remains foundational, but putting and distance play a more visible role in differentiating players at the very top.\n\nOverall, the supervised learning section shows:\n\n-   Scoring average is highly explainable and strongly driven by tee-to-green performance.\n\n-   Winning is much harder to predict, but still leans heavily on long-game quality, with meaningful contributions from putting and distance.\n\n```{python}\nimportances_clf = pd.Series(\nrf_clf.feature_importances_,\nindex=clf_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_clf.values, y=importances_clf.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Win)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_wins.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_clf\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"embed-resources":true,"output-file":"supervised.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":"cosmo","smooth-scroll":true,"title":"Supervised Learning","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}