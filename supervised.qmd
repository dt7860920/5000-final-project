---
title: "Supervised Learning"
format: 
  html:
    toc: true
    code-fold: true
    embed-resources: true
editor: visual
---

In this section, I use supervised learning to answer four main research questions:

> -   Which performance metrics best predict scoring average on the PGA Tour?
>
> -   What distinguishes winning players from the rest of the field?
>
> -   Is modern success driven more by strokes-gained tee-to-green or putting?
>
> -   Does distance directly contribute to better score or titles won when compared to strokes-gain metrics?

I fit both regression models (for scoring average) and classification models (for whether a player wins at least one event in a season) and compare their performance and feature importance.

### Setup

```{python}
import pandas as pd
import numpy as np
from sklearn.metrics import root_mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import (
    mean_squared_error,
    r2_score,
    accuracy_score,
    roc_auc_score,
    classification_report,
)
```

### Load data

```{python}
pga = pd.read_csv("data/processed-data/pga_cleaned.csv")
pga.head()
pga.columns
```

### Regression (Predict Score)

The first supervised task is predicting scoring average using only a few core performance metrics:

-   **Driving Distance**

-   **Greens in Regulation % (GIR%)**

-   **Strokes-Gained Putting (SG_P)**

-   **Strokes-Gained Tee-to-Green (SG_TTG)**

These four features represent the main components of performance: distance, approach/accuracy, putting, and long-game quality.

Variables defined:

-   `X_reg` = the matrix of these four predictors

-   `y_reg` = the target variable `scoring`

Rows with missing values were dropped to ensure the regression model has complete data for all inputs.

```{python}
reg_features = [
  "drive_distance",
  "gir_pct",
  "sg_p",
  "sg_ttg",
]
reg_target = "scoring"

reg_df = pga[reg_features + [reg_target]].dropna()
X_reg = reg_df[reg_features]
y_reg = reg_df[reg_target]

X_reg.shape, y_reg.shape
```

### Training and testing split (Predict Score)

To properly evaluate the regression model, I split the data into:

-   **80% training data** – used to fit the model

-   **20% testing data** – held out for evaluation

I also standardize the predictors (mean 0, variance 1) using StandardScaler. This is especially important for Linear Regression, which can be sensitive to feature scales, and makes coefficients more interpretable in standardized units.

```{python}
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
X_reg, y_reg, test_size=0.2, random_state=42
)

scaler_reg = StandardScaler()
X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)
X_test_reg_scaled = scaler_reg.transform(X_test_reg)
```

### Linear Regression (Predict Score)

In this step, I fit a linear regression model on the scaled training data and evaluate it on the test data.

-   **RMSE** (root mean squared error) measures the typical prediction error in strokes.

-   **R²** measures how much of the variance in scoring average is explained by the four predictors.

The results:

-   RMSE ≈ 0.19

-   R² ≈ 0.91

This means the model explains about **91%** of the variation in scoring averages. That is extremely strong for sports performance data, suggesting that these four metrics capture most of what drives scoring differences between players.

```{python}
lin_reg = LinearRegression()
lin_reg.fit(X_train_reg_scaled, y_train_reg)

y_pred_lin = lin_reg.predict(X_test_reg_scaled)

rmse_lin = root_mean_squared_error(y_test_reg, y_pred_lin)
r2_lin = r2_score(y_test_reg, y_pred_lin)

print("Linear Regression (Predict Score)")
print(" RMSE:", rmse_lin)
print(" R^2 :", r2_lin)
```

### Random Forest (Predict Score)

Unlike linear regression, a random forest can automatically capture nonlinearities and interactions between variables.

The results:

-   RMSE ≈ 0.21

-   R² ≈ 0.90

This shows that:

-   The relationship between scoring and these features is mostly linear, since the simpler linear model actually performs slightly better.

-   Random Forest is still useful for its feature importance output, which helps quantify which predictors matter most.

```{python}
rf_reg = RandomForestRegressor(
  n_estimators=300,
  random_state=42,
  max_depth=None,
  n_jobs=-1
)
rf_reg.fit(X_train_reg, y_train_reg)

y_pred_rf = rf_reg.predict(X_test_reg)

rmse_rf = root_mean_squared_error(y_test_reg, y_pred_rf)
r2_rf = r2_score(y_test_reg, y_pred_rf)

print("Random Forest Regression (Predict Score)")
print("  RMSE:", rmse_rf)
print("  R^2 :", r2_rf)
```

### Feature Importance (Predict Score)

Here, I extract feature importance values from the random forest regression model and visualize them as a bar chart.

The ranking:

1.  **SG_TTG**

2.  **SG_P**

3.  **GIR%**

4.  **Driving Distance**

**Interpretation:**

-   Strokes Gained Tee-to-Green dominates scoring prediction.

-   Putting still matters, but contributes less than overall long-game quality.

-   Distance is the least important of the four, once strokes gained metrics are included.

This matches both the EDA and existing literature: scoring is primarily driven by tee-to-green performance, not just hitting it far.

```{python}
importances_reg = pd.Series(
rf_reg.feature_importances_,
index=reg_features
).sort_values(ascending=False)

plt.figure(figsize=(8,5))
sns.barplot(x=importances_reg.values, y=importances_reg.index)
plt.xlabel("Feature Importance")
plt.title("RF Feature Importance (Predict Score)")
plt.tight_layout()
plt.savefig("images/supervised_rf_importance_scoring.png", dpi=300, bbox_inches="tight")
plt.show()

importances_reg
```

### Classification Model (Predict Win)

Now I switch from predicting a continuous outcome (scoring) to a binary outcome: whether a player wins at least one event in a season.

I create:

-   `has_win = 1` if `win > 0`

-   `has_win = 0` otherwise

The same four features are used:

-   **Driving Distance**

-   **Greens in Regulation % (GIR%)**

-   **Strokes-Gained Putting (SG_P)**

-   **Strokes-Gained Tee-to-Green (SG_TTG)**

```{python}

pga["has_win"] = (pga["win"] > 0).astype(int)

clf_features = [
"drive_distance",
"gir_pct",
"sg_p",
"sg_ttg",
]

clf_df = pga[clf_features + ["has_win"]].dropna()
X_clf = clf_df[clf_features]
y_clf = clf_df["has_win"]

X_clf.shape, y_clf.value_counts()
```

### Training and testing split (Predict Win)

Similar to the regression case, I:

-   Split into training (80%) and test (20%) sets

-   Use `stratify=y_clf` so the class balance (winner vs non-winner) is preserved

-   Standardize predictors for logistic regression

This setup supports fair model evaluation, especially important because only a small percentage of seasons include a win, which makes the outcome imbalanced.

```{python}

X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf
)

scaler_clf = StandardScaler()
X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)
X_test_clf_scaled = scaler_clf.transform(X_test_clf)
```

### Logistic Regression (Predict Win)

I then fit a Logistic Regression model on the scaled training data and evaluate it on the test set.

Metrics:

-   **Accuracy** – overall percent of correct predictions

-   **ROC AUC** – how well the model ranks winners above non-winners

-   **classification_report** – precision/recall/F1 for winners vs non-winners

The accuracy is high (\~0.88), but recall for the winning class is low. This reflects reality: winning is rare and noisy, and many seasons with strong stats still result in no wins.

```{python}
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_clf_scaled, y_train_clf)

y_pred_log = log_reg.predict(X_test_clf_scaled)
y_prob_log = log_reg.predict_proba(X_test_clf_scaled)[:, 1]

acc_log = accuracy_score(y_test_clf, y_pred_log)
auc_log = roc_auc_score(y_test_clf, y_prob_log)

print("Logistic Regression (Predict Win)")
print("  Accuracy:", acc_log)
print("  ROC AUC :", auc_log)
print(classification_report(y_test_clf, y_pred_log))
```

### Random Forest (Predict Win)

I also train a Random Forest Classifier with 400 trees. Again, this model is less sensitive to scaling and can capture non-linear decision boundaries.

Evaluation shows:

-   Accuracy around 0.88

-   ROC AUC around 0.71

-   Improved but still modest recall for winners

This confirms that winning is harder to predict than scoring. Tournament victories depend on many factors not captured in season aggregates (course fit, weather, clutch performance, luck, etc.).

```{python}
rf_clf = RandomForestClassifier(
n_estimators=400,
random_state=42,
max_depth=None,
n_jobs=-1
)
rf_clf.fit(X_train_clf, y_train_clf)

y_pred_rf_clf = rf_clf.predict(X_test_clf)
y_prob_rf_clf = rf_clf.predict_proba(X_test_clf)[:, 1]

acc_rf = accuracy_score(y_test_clf, y_pred_rf_clf)
auc_rf = roc_auc_score(y_test_clf, y_prob_rf_clf)

print("Random Forest Classifier (Predict Win)")
print("  Accuracy:", acc_rf)
print("  ROC AUC :", auc_rf)
print(classification_report(y_test_clf, y_pred_rf_clf))
```

### Feature Importance (Predict Win)

Finally, I extract feature importance from the random forest classifier and visualize it.

The ranking tends to be:

-   **SG_TTG** – still the top predictor

-   **GIR%**, **Driving Distance**, and **SG_P** contribute at a similar level

**Interpretation:**

-   Winning a tournament requires a complete game: long-game strength, solid GIR%, putting, and distance all matter.

-   SG_TTG remains foundational, but putting and distance play a more visible role in differentiating players at the very top.

Overall, the supervised learning section shows:

-   **Scoring average** is highly explainable and strongly driven by tee-to-green performance.

-   **Winning** is much harder to predict, but still leans heavily on long-game quality, with meaningful contributions from putting and distance.

```{python}
importances_clf = pd.Series(
rf_clf.feature_importances_,
index=clf_features
).sort_values(ascending=False)

plt.figure(figsize=(8,5))
sns.barplot(x=importances_clf.values, y=importances_clf.index)
plt.xlabel("Feature Importance")
plt.title("RF Feature Importance (Predict Win)")
plt.tight_layout()
plt.savefig("images/supervised_rf_importance_wins.png", dpi=300, bbox_inches="tight")
plt.show()

importances_clf
```
