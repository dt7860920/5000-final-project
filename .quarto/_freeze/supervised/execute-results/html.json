{
  "hash": "4209b81ef2c503c1bce57cf725c91766",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Supervised Learning\"\nformat: \n  html:\n    toc: true\n    code-fold: true\n    embed-resources: true\neditor: visual\n---\n\nIn this section, I use supervised learning to answer four main research questions:\n\n> -   Which performance metrics best predict scoring average on the PGA Tour?\n>\n> -   What distinguishes winning players from the rest of the field?\n>\n> -   Is modern success driven more by strokes-gained tee-to-green or putting?\n>\n> -   Does distance directly contribute to better score or titles won when compared to strokes-gain metrics?\n\nI fit both regression models (for scoring average) and classification models (for whether a player wins at least one event in a season) and compare their performance and feature importance.\n\n### Setup\n\n::: {#e54b0d92 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import root_mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n    accuracy_score,\n    roc_auc_score,\n    classification_report,\n)\n```\n:::\n\n\n### Load data\n\n::: {#558bdc45 .cell execution_count=2}\n``` {.python .cell-code}\npga = pd.read_csv(\"data/processed-data/pga_cleaned.csv\")\npga.head()\npga.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nIndex(['name', 'year', 'country', 'scoring', 'drive_distance', 'gir_pct',\n       'sg_p', 'sg_ttg', 'sg_t', 'top_10', 'win'],\n      dtype='object')\n```\n:::\n:::\n\n\n### Regression (Predict Score)\n\nThe first supervised task is predicting scoring average using only a few core performance metrics:\n\n-   **Driving Distance**\n\n-   **Greens in Regulation % (GIR%)**\n\n-   **Strokes-Gained Putting (SG_P)**\n\n-   **Strokes-Gained Tee-to-Green (SG_TTG)**\n\nThese four features represent the main components of performance: distance, approach/accuracy, putting, and long-game quality.\n\nVariables defined:\n\n-   `X_reg` = the matrix of these four predictors\n\n-   `y_reg` = the target variable `scoring`\n\nRows with missing values were dropped to ensure the regression model has complete data for all inputs.\n\n::: {#e76c4ee8 .cell execution_count=3}\n``` {.python .cell-code}\nreg_features = [\n  \"drive_distance\",\n  \"gir_pct\",\n  \"sg_p\",\n  \"sg_ttg\",\n]\nreg_target = \"scoring\"\n\nreg_df = pga[reg_features + [reg_target]].dropna()\nX_reg = reg_df[reg_features]\ny_reg = reg_df[reg_target]\n\nX_reg.shape, y_reg.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n((2747, 4), (2747,))\n```\n:::\n:::\n\n\n### Training and testing split (Predict Score)\n\nTo properly evaluate the regression model, I split the data into:\n\n-   **80% training data** – used to fit the model\n\n-   **20% testing data** – held out for evaluation\n\nI also standardize the predictors (mean 0, variance 1) using StandardScaler. This is especially important for Linear Regression, which can be sensitive to feature scales, and makes coefficients more interpretable in standardized units.\n\n::: {#d49ab8e1 .cell execution_count=4}\n``` {.python .cell-code}\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\nX_reg, y_reg, test_size=0.2, random_state=42\n)\n\nscaler_reg = StandardScaler()\nX_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\nX_test_reg_scaled = scaler_reg.transform(X_test_reg)\n```\n:::\n\n\n### Linear Regression (Predict Score)\n\nIn this step, I fit a linear regression model on the scaled training data and evaluate it on the test data.\n\n-   **RMSE** (root mean squared error) measures the typical prediction error in strokes.\n\n-   **R²** measures how much of the variance in scoring average is explained by the four predictors.\n\nThe results:\n\n-   RMSE ≈ 0.19\n\n-   R² ≈ 0.91\n\nThis means the model explains about **91%** of the variation in scoring averages. That is extremely strong for sports performance data, suggesting that these four metrics capture most of what drives scoring differences between players.\n\n::: {#9ab5f0eb .cell execution_count=5}\n``` {.python .cell-code}\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_reg_scaled, y_train_reg)\n\ny_pred_lin = lin_reg.predict(X_test_reg_scaled)\n\nrmse_lin = root_mean_squared_error(y_test_reg, y_pred_lin)\nr2_lin = r2_score(y_test_reg, y_pred_lin)\n\nprint(\"Linear Regression (Predict Score)\")\nprint(\" RMSE:\", rmse_lin)\nprint(\" R^2 :\", r2_lin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression (Predict Score)\n RMSE: 0.1919919438714076\n R^2 : 0.9098003470553254\n```\n:::\n:::\n\n\n### Random Forest (Predict Score)\n\nUnlike linear regression, a random forest can automatically capture nonlinearities and interactions between variables.\n\nThe results:\n\n-   RMSE ≈ 0.21\n\n-   R² ≈ 0.90\n\nThis shows that:\n\n-   The relationship between scoring and these features is mostly linear, since the simpler linear model actually performs slightly better.\n\n-   Random Forest is still useful for its feature importance output, which helps quantify which predictors matter most.\n\n::: {#580acc0c .cell execution_count=6}\n``` {.python .cell-code}\nrf_reg = RandomForestRegressor(\n  n_estimators=300,\n  random_state=42,\n  max_depth=None,\n  n_jobs=-1\n)\nrf_reg.fit(X_train_reg, y_train_reg)\n\ny_pred_rf = rf_reg.predict(X_test_reg)\n\nrmse_rf = root_mean_squared_error(y_test_reg, y_pred_rf)\nr2_rf = r2_score(y_test_reg, y_pred_rf)\n\nprint(\"Random Forest Regression (Predict Score)\")\nprint(\"  RMSE:\", rmse_rf)\nprint(\"  R^2 :\", r2_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Regression (Predict Score)\n  RMSE: 0.20574189715394375\n  R^2 : 0.8964179897284077\n```\n:::\n:::\n\n\n### Feature Importance (Predict Score)\n\nHere, I extract feature importance values from the random forest regression model and visualize them as a bar chart.\n\nThe ranking:\n\n1.  **SG_TTG**\n\n2.  **SG_P**\n\n3.  **GIR%**\n\n4.  **Driving Distance**\n\n**Interpretation:**\n\n-   Strokes Gained Tee-to-Green dominates scoring prediction.\n\n-   Putting still matters, but contributes less than overall long-game quality.\n\n-   Distance is the least important of the four, once strokes gained metrics are included.\n\nThis matches both the EDA and existing literature: scoring is primarily driven by tee-to-green performance, not just hitting it far.\n\n::: {#b4a31c47 .cell execution_count=7}\n``` {.python .cell-code}\nimportances_reg = pd.Series(\nrf_reg.feature_importances_,\nindex=reg_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_reg.values, y=importances_reg.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Score)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_scoring.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_reg\n```\n\n::: {.cell-output .cell-output-display}\n![](supervised_files/figure-html/cell-8-output-1.png){width=757 height=468}\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nsg_ttg            0.726565\nsg_p              0.233879\ngir_pct           0.020768\ndrive_distance    0.018789\ndtype: float64\n```\n:::\n:::\n\n\n### Classification Model (Predict Win)\n\nNow I switch from predicting a continuous outcome (scoring) to a binary outcome: whether a player wins at least one event in a season.\n\nI create:\n\n-   `has_win = 1` if `win > 0`\n\n-   `has_win = 0` otherwise\n\nThe same four features are used:\n\n-   **Driving Distance**\n\n-   **Greens in Regulation % (GIR%)**\n\n-   **Strokes-Gained Putting (SG_P)**\n\n-   **Strokes-Gained Tee-to-Green (SG_TTG)**\n\n::: {#ef5a10d2 .cell execution_count=8}\n``` {.python .cell-code}\npga[\"has_win\"] = (pga[\"win\"] > 0).astype(int)\n\nclf_features = [\n\"drive_distance\",\n\"gir_pct\",\n\"sg_p\",\n\"sg_ttg\",\n]\n\nclf_df = pga[clf_features + [\"has_win\"]].dropna()\nX_clf = clf_df[clf_features]\ny_clf = clf_df[\"has_win\"]\n\nX_clf.shape, y_clf.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n((2747, 4),\n has_win\n 0    2410\n 1     337\n Name: count, dtype: int64)\n```\n:::\n:::\n\n\n### Training and testing split (Predict Win)\n\nSimilar to the regression case, I:\n\n-   Split into training (80%) and test (20%) sets\n\n-   Use `stratify=y_clf` so the class balance (winner vs non-winner) is preserved\n\n-   Standardize predictors for logistic regression\n\nThis setup supports fair model evaluation, especially important because only a small percentage of seasons include a win, which makes the outcome imbalanced.\n\n::: {#13a7eaa2 .cell execution_count=9}\n``` {.python .cell-code}\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\nX_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n)\n\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n```\n:::\n\n\n### Logistic Regression (Predict Win)\n\nI then fit a Logistic Regression model on the scaled training data and evaluate it on the test set.\n\nMetrics:\n\n-   **Accuracy** – overall percent of correct predictions\n\n-   **ROC AUC** – how well the model ranks winners above non-winners\n\n-   **classification_report** – precision/recall/F1 for winners vs non-winners\n\nThe accuracy is high (\\~0.88), but recall for the winning class is low. This reflects reality: winning is rare and noisy, and many seasons with strong stats still result in no wins.\n\n::: {#6568eb55 .cell execution_count=10}\n``` {.python .cell-code}\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train_clf_scaled, y_train_clf)\n\ny_pred_log = log_reg.predict(X_test_clf_scaled)\ny_prob_log = log_reg.predict_proba(X_test_clf_scaled)[:, 1]\n\nacc_log = accuracy_score(y_test_clf, y_pred_log)\nauc_log = roc_auc_score(y_test_clf, y_prob_log)\n\nprint(\"Logistic Regression (Predict Win)\")\nprint(\"  Accuracy:\", acc_log)\nprint(\"  ROC AUC :\", auc_log)\nprint(classification_report(y_test_clf, y_pred_log))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression (Predict Win)\n  Accuracy: 0.8818181818181818\n  ROC AUC : 0.7675906183368869\n              precision    recall  f1-score   support\n\n           0       0.88      1.00      0.94       483\n           1       0.75      0.04      0.08        67\n\n    accuracy                           0.88       550\n   macro avg       0.82      0.52      0.51       550\nweighted avg       0.87      0.88      0.83       550\n\n```\n:::\n:::\n\n\n### Random Forest (Predict Win)\n\nI also train a Random Forest Classifier with 400 trees. Again, this model is less sensitive to scaling and can capture non-linear decision boundaries.\n\nEvaluation shows:\n\n-   Accuracy around 0.88\n\n-   ROC AUC around 0.71\n\n-   Improved but still modest recall for winners\n\nThis confirms that winning is harder to predict than scoring. Tournament victories depend on many factors not captured in season aggregates (course fit, weather, clutch performance, luck, etc.).\n\n::: {#d88b4929 .cell execution_count=11}\n``` {.python .cell-code}\nrf_clf = RandomForestClassifier(\nn_estimators=400,\nrandom_state=42,\nmax_depth=None,\nn_jobs=-1\n)\nrf_clf.fit(X_train_clf, y_train_clf)\n\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_prob_rf_clf = rf_clf.predict_proba(X_test_clf)[:, 1]\n\nacc_rf = accuracy_score(y_test_clf, y_pred_rf_clf)\nauc_rf = roc_auc_score(y_test_clf, y_prob_rf_clf)\n\nprint(\"Random Forest Classifier (Predict Win)\")\nprint(\"  Accuracy:\", acc_rf)\nprint(\"  ROC AUC :\", auc_rf)\nprint(classification_report(y_test_clf, y_pred_rf_clf))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Classifier (Predict Win)\n  Accuracy: 0.88\n  ROC AUC : 0.7098668149933561\n              precision    recall  f1-score   support\n\n           0       0.89      0.99      0.94       483\n           1       0.54      0.10      0.17        67\n\n    accuracy                           0.88       550\n   macro avg       0.71      0.55      0.56       550\nweighted avg       0.85      0.88      0.84       550\n\n```\n:::\n:::\n\n\n### Feature Importance (Predict Win)\n\nFinally, I extract feature importance from the random forest classifier and visualize it.\n\nThe ranking tends to be:\n\n-   SG_TTG – still the top predictor\n\n-   GIR%, Driving Distance, and SG_P contribute at a similar level\n\n**Interpretation:**\n\n-   Winning a tournament requires a complete game: long-game strength, solid GIR%, putting, and distance all matter.\n\n-   SG_TTG remains foundational, but putting and distance play a more visible role in differentiating players at the very top.\n\nOverall, the supervised learning section shows:\n\n-   Scoring average is highly explainable and strongly driven by tee-to-green performance.\n\n-   Winning is much harder to predict, but still leans heavily on long-game quality, with meaningful contributions from putting and distance.\n\n::: {#1282f4d4 .cell execution_count=12}\n``` {.python .cell-code}\nimportances_clf = pd.Series(\nrf_clf.feature_importances_,\nindex=clf_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_clf.values, y=importances_clf.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Win)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_wins.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_clf\n```\n\n::: {.cell-output .cell-output-display}\n![](supervised_files/figure-html/cell-13-output-1.png){width=757 height=468}\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nsg_ttg            0.298724\ngir_pct           0.239733\ndrive_distance    0.231139\nsg_p              0.230404\ndtype: float64\n```\n:::\n:::\n\n\n",
    "supporting": [
      "supervised_files"
    ],
    "filters": [],
    "includes": {}
  }
}