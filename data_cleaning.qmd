---
title: "Data Cleaning"
format:
  html:
    toc: true
    code-fold: true
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
---

### Load Data:

Data cleaning starts by loading two separate PGA Tour datasets from different points in time (both from GitHub repositories). The first dataset covers 2007–2017, and the second covers 2017–2022. The two files differ significantly in formatting, structure, and naming conventions because they come from various authors and scraping pipelines. Loading the data into memory is the foundation for applying systematic cleaning operations that will ultimately allow these datasets to be merged into a single analytical dataframe.

The use of explicit file paths and controlled encoding ensures that the datasets are read consistently across machines and avoids issues with non-standard characters. This stage introduces the raw structure of the data, but no analysis can proceed until the numerous inconsistencies between the datasets are resolved.

```{python}
import pandas as pd
import numpy as np

raw_path = "data/raw-data/pgatour_raw.csv"
full_path = "data/raw-data/pga_full.csv"

df_raw = pd.read_csv(raw_path, encoding="latin1")
df_full = pd.read_csv(full_path, encoding="latin1")
```

### Helper Function

To address the inconsistencies present across the two sources, several custom helper functions are created. These functions standardize the formatting of column names, numerical fields, and player names.

**Clean column names:**

-   Converts names to lowercase

-   Replaces spaces with underscores

-   Removes stray punctuation

-   Standardizes percent signs (%)

**Clean numeric data:**

-   Commas

-   Dollar signs

-   Percentage symbols

**Clean player name:**

-   Extra spaces

-   Commas

-   Inconsistent capitalization

```{python}

def clean_column_names(df):
    df = df.copy()
    df.columns = (
        df.columns
        .str.strip()
        .str.lower()
        .str.replace(" ", "_")
        .str.replace("%", "pct")
        .str.replace(".", "_")
    )
    return df

def clean_numeric(series):
    return (
        series.astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("$", "", regex=False)
        .str.replace("%", "", regex=False)
        .replace("", np.nan)
        .astype(float)
    )

def clean_player_name(name):
    name = str(name)
    name = name.replace(",", "").strip()
    return name.title()
```

### Standardize Player Names

Both datasets include a “name” column, but inconsistencies in formatting make direct merging unreliable. By applying the **clean_player_name** function to both dataframes, the pipeline ensures that names follow a consistent title-case style without extraneous punctuation. Since the analysis treats each player-season individually, accurate name alignment is critical. While this step does not fully resolve ambiguities between players with identical names, it significantly reduces noise and prepares the data for merging.

```{python}
if "name" in df_raw.columns:
    df_raw["name"] = df_raw["name"].apply(clean_player_name)
if "name" in df_full.columns:
    df_full["name"] = df_full["name"].apply(clean_player_name)
```

### Fix Numeric Columns

Next, numerical fields such as scoring average, driving distance, greens in regulation percentage, and strokes-gained metrics are cleaned using the **clean_numeric** function. These fields appear in string form in the raw data, often containing formatting characters that obstruct quantitative analysis.

By applying this cleaning operation to every numeric column across both datasets, the result is a structurally consistent numeric dataset suitable for statistical modeling. Additionally, the “year” column is explicitly converted to integer type to allow proper sorting and grouping by season in later stages.

```{python}
numeric_raw = [
    "rounds", "scoring", "drive_distance",
    "gir_pct", "sg_p", "sg_ttg", "sg_t",
    "top_10", "win"
]
numeric_full = [
    "rounds", "scoring", "birdie_avg", "drive_distance",
    "gir_pct", "sg_p", "sg_ttg", "sg_t",
    "top_10", "win"
]

for col in numeric_raw:
    if col in df_raw.columns:
        df_raw[col] = clean_numeric(df_raw[col])

for col in numeric_full:
    if col in df_full.columns:
        df_full[col] = clean_numeric(df_full[col])

# Year should be numeric
for df in [df_raw, df_full]:
    if "year" in df.columns:
        df["year"] = pd.to_numeric(df["year"], errors="coerce").astype("Int64")

```

### Standardize Column Names

The two datasets contain overlapping variables, but the names of these variables differ significantly. For example, the newer dataset uses names like “ScoringAvg,” “BirdieAvg,” and “DrivingDistance,” while the older dataset uses lowercase snake_case fields.

This section of the cleaning process unifies all variable names into a single naming system. Columns are renamed to ensure consistency in metrics such as strokes gained (SG_P, SG_TTG, SG_T), GIR percentage, and performance indicators like top-10 finishes and wins.

The older dataset also includes several column variations representing wins (e.g., “1st,” “1_st,” “1st\_”), which must be consolidated into the standard “win” field.

```{python}
df_raw = clean_column_names(df_raw)
df_full = clean_column_names(df_full)

df_full = df_full.rename(columns={
    "player": "name",
    "scoringavg": "scoring",
    "birdieavg": "birdie_avg",
    "drivingdistance": "drive_distance",
    "gir": "gir_pct",
    "girpct": "gir_pct",
    "sg_putting": "sg_p",
    "sg_total": "sg_t",
    "sg_teetogreen": "sg_ttg",
    "top10finishes": "top_10",
    "wins": "win"
})

# ---- pgatour_raw.csv ----
df_raw = df_raw.rename(columns={
    "top_10": "top_10",
    "1st": "win",
    "1_st": "win",
    "1st_": "win",
    "gir__pct": "gir_pct",   # depending on how the % got mapped
    "gir_pct": "gir_pct",
    "fwy__pct": "fwy_pct",
    "fwy_pct": "fwy_pct"
})
```

### Select Overlapping Features in both Dataset

Because the two datasets were scraped and structured differently, not all variables appear in both. To create a consistent merged dataframe, we identify a list of standardized “core features” that appear in both datasets or are essential to the analysis.

Any missing columns are added as empty fields so that both datasets share identical structures. This alignment step ensures that concatenating the datasets will not introduce misaligned columns or improperly shifted values.

```{python}
standard_cols = [
    "name", "year", "country",
    "scoring",
    "drive_distance", "gir_pct",
    "sg_p", "sg_ttg", "sg_t",
    "top_10", "win"
]

for col in standard_cols:
    if col not in df_raw.columns:
        df_raw[col] = pd.NA
    if col not in df_full.columns:
        df_full[col] = pd.NA

df_raw_std = df_raw[standard_cols]
df_full_std = df_full[standard_cols]
```

### Data Merging

With standardized formatting and aligned columns, the two datasets are merged using **pd.concat**. This produces a unified dataset of player-season observations covering 2007–2022.

However, several additional challenges arise:

-   Many players have missing values for wins or top-10 finishes, which can interfere with classification models. To reduce this issue, missing values are replaced with 0.

-   Duplicate entries sometimes appear for the same player-year. To retain the most complete record, rows are ranked by the number of non-missing values, and the most informative entry is kept.

```{python}
combined = pd.concat([df_raw_std, df_full_std], ignore_index=True)

combined["win"] = combined["win"].fillna(0).astype("Int64")
combined["top_10"] = combined["top_10"].fillna(0).astype("Int64")

combined["non_na_count"] = combined.notna().sum(axis=1)

combined = (
    combined.sort_values(["name", "year", "non_na_count"])
            .drop_duplicates(subset=["name", "year"], keep="last")
            .drop(columns="non_na_count")
)

print("Final shape:", combined.shape)
print(combined.head())
```

## Save Merged Data

The cleaned dataset is saved as **pga_cleaned.csv** for reuse across EDA, clustering, and supervised learning. This ensures reproducibility and prevents re-running the entire cleaning pipeline each time a new analysis file is executed.

```{python}
output_path = "data/processed-data/pga_cleaned.csv"
combined.to_csv(output_path, index=False)
output_path
```

## Before & After Comparison

Finally, the cleaned dataset is compared against the original sources to verify that:

-   Variable names were correctly standardized

-   Numeric values were converted properly

-   Merging did not introduce unintended duplicates

-   Missing values were handled as intended

This step confirms the integrity of the cleaning process before downstream modeling occurs.

```{python}
df_raw.head()
df_full.head()
combined.head()
```

# Limitations on data cleaning:

-   Replacing missing wins and top 10s with zero can introduce bias
-   Removing characters like % and \$ in the numeric columns may loses context
-   Missing data imputation was not used, leaving missing stats as NaN
-   Player names lack unique identifiers, meaning identification errors may still occur for players with similar names.
