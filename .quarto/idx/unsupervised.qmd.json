{"title":"Unsupervised Learning","markdown":{"yaml":{"title":"Unsupervised Learning","format":{"html":{"toc":true,"code-fold":true,"embed-resources":true}},"execute":{"echo":true,"warning":false,"message":false},"editor":"visual"},"headingText":"Data Import","containsRefs":false,"markdown":"\n\nThe goal of this section is to use **unsupervised learning** to discover natural player “types” on the PGA Tour, based purely on performance statistics rather than predefined labels. Instead of asking “who won?” or “who scored lowest?” we ask:\n\n> Are there identifiable types of PGA Tour players using unsupervised learning?\n\nTo answer this, I apply K-Means clustering to season-level player performance and then interpret the resulting clusters using both summary statistics and example players from each group.\n\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npga = pd.read_csv(\"data/processed-data/pga_cleaned.csv\")\npga.head()\n```\n\n### Filter Features\n\nClustering performance profiles requires choosing a set of features that capture how players **actually play golf**, not just whether they happen to win in a given season. For this reason, I include the following variables:\n\n-   **scoring** – scoring average (strokes per round)\n\n-   **drive_distance** – average driving distance\n\n-   **gir_pct** – greens in regulation percentage\n\n-   **sg_p** – strokes gained putting\n\n-   **sg_ttg** – strokes gained tee-to-green\n\n-   **sg_t** – strokes gained total\n\n-   **top_10** – number of top-10 finishes in the season\n\n-   **win** – number of wins in the season\n\nTogether, these variables summarize scoring outcome, long-game and short-game performance, and high-end results (top-10s and wins).\n\nBefore fitting the model, I drop any rows with missing values in these columns to ensure the clustering algorithm has complete data for each player-season.\n\n```{python}\nfeatures = [\n    \"scoring\",\n    \"drive_distance\",\n    \"gir_pct\",\n    \"sg_p\",\n    \"sg_ttg\",\n    \"sg_t\",\n    \"top_10\",\n    \"win\"\n]\n\nX = pga[features].dropna()\nX.head()\n```\n\n### Scale\n\nBecause these features are on different scales (for example, scoring is around 70, sg_ttg tends to be between roughly −3 and +3, and top_10 is a small integer), I standardize all selected features using StandardScaler.\n\nStandardization transforms each variable to have:\n\n-   Mean ≈ 0\n\n    Standard deviation ≈ 1\n\nThis is especially important for K-Means, which uses Euclidean distance in feature space. Without scaling, variables with larger numerical ranges (like scoring or driving distance) would dominate the distance computation and overshadow more subtle but important statistics like strokes gained.\n\n```{python}\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n\n### Calculate optimal K mean\n\nTo determine a reasonable number of clusters, I use the Elbow Method.\n\nFor k from 2 to 9, I:\n\n1.  Fit a K-Means model with `n_clusters = k` on the scaled data\n\n2.  Record the **inertia** (within-cluster sum of squared distances)\n\nPlotting inertia against k typically shows a rapidly decreasing curve that then begins to “bend” or “flatten” — the “elbow.” That elbow represents a good trade-off between model complexity (more clusters) and compactness (lower within-cluster variance).\n\nIn this case, the elbow occurs around k = 4, suggesting that four distinct performance archetypes are present in the data. This choice balances interpretability and segmentation detail.\n\n```{python}\ninertia = []\nK_range = range(2, 10)\n\nfor k in K_range:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X_scaled)\n    inertia.append(km.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.plot(K_range, inertia, marker='o')\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Inertia (Within-Cluster SSE)\")\nplt.title(\"Elbow Method for Optimal k\")\nplt.grid(True)\nplt.show()\n\n```\n\n### Set K mean as 4\n\nAfter selecting **k = 4**, I fit a K-Means model with:\n\n-   `n_clusters = 4`\n\n-   `random_state = 42`\n\n-   `n_init = 10`\n\nThe model assigns each player-season to one of four clusters, stored in a new column `cluster`. I then create a `pga_clusters` dataframe, which contains all original variables for the filtered rows along with the cluster label.\n\nThis allows me to:\n\n-   Summarize each cluster by averaging statistics\n\n-   Look up specific players in each cluster\n\n-   Cross-reference cluster labels with scoring, wins, and strokes-gained metrics\n\n```{python}\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_scaled)\n\npga_clusters = pga.loc[X.index].copy()\npga_clusters[\"cluster\"] = clusters\n```\n\n### PCA graph\n\nTo understand how the clusters are arranged in high-dimensional space, I apply Principal Component Analysis (PCA) to reduce the standardized feature space to two components (PC1 and PC2).\n\nPCA serves two purposes here:\n\n1.  It compresses information from 8 features into 2 dimensions for visualization.\n\n2.  It preserves as much variance as possible, so the clusters remain meaningfully separated in the 2D plot.\n\nEach player-season is projected onto the (PC1, PC2) plane, and the points are colored by cluster label. The resulting scatterplot:\n\n-   Shows clearly separated groupings, confirming that the K-Means clustering captures real structure in the data.\n\n-   Provides a visual intuition for how different performance profiles occupy different regions of the feature space (e.g., elite ball-strikers vs. struggling players).\n\n```{python}\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\ncoords = pca.fit_transform(X_scaled)\n\npga_clusters[\"PC1\"] = coords[:,0]\npga_clusters[\"PC2\"] = coords[:,1]\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=pga_clusters,\n    x=\"PC1\", y=\"PC2\",\n    hue=\"cluster\", palette=\"tab10\", alpha=0.6\n)\nplt.title(\"PCA Visualization of Player Clusters\")\nplt.savefig(\"images/pca_clusters.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n```\n\n### Cluster Profile\n\n**Cluster 0 - Balanced Professionals**\n\n-   Average SG metrics\n\n-   Solid but not outstanding in any single category\n\n-   Mid-level scoring average\n\n**Cluster 1 - Struggling Players**\n\n-   Negative SG_TTG and SG_T\n\n-   Higher scoring average\n\n-   Few top-10 finishes\n\n**Cluster 2 - Elite Ball Strikers**\n\n-   Highest SG_TTG\n\n-   Lowest scoring average\n\n-   Best combination of GIR%, distance, and SG_TTG\n\n-   Most number of wins and top-10s\n\n**Cluster 3 - Elite Putters**\n\n-   Highest SG_P\n\n-   Moderate SG_TTG\n\n-   Relies heavily on short-game performance\n\n```{python}\ncluster_profile = pga_clusters.groupby(\"cluster\")[features].mean().round(2)\ncluster_profile\n```\n\n### Cluster player names\n\nTo make the clusters more concrete and interpretable, I display a few representative players for each cluster.\n\nFor each cluster:\n\n1.  Filter by cluster label\n\n2.  Sort by year and scoring\n\n3.  Drop duplicate names so each player appears once\n\n4.  Show a small number of players with their scoring, SG_TTG, SG_P, top-10s, and wins\n\nThis yields intuitive examples such as:\n\n-   **Cluster 0 (Balanced Professionals):** players like Anthony Kim, Jeff Overton, Kenny Perry\n\n-   **Cluster 1 (Struggling Players):** players like Anders Hansen, Glen Day, Gavin Coles\n\n-   **Cluster 2 (Elite Ball-Strikers):** players like Tiger Woods, Ernie Els, Justin Rose\n\n-   **Cluster 3 (Elite Putters):** players like Arron Oberholser, Padraig Harrington, Tim Clark\n\nBy combining cluster averages and named examples, the analysis shows that unsupervised learning successfully recovers meaningful player archetypes that correspond to real-world styles and reputations on the PGA Tour.\n\n```{python}\npga_clusters = pga.loc[X.index].copy()\npga_clusters[\"cluster\"] = clusters\n\n\ndef show_players(df, cluster_id, sort_col=\"scoring\", n=15):\n    return (\n        df[df[\"cluster\"] == cluster_id]\n        .sort_values(sort_col)\n        [[\"name\", \"year\", \"scoring\", \"sg_ttg\", \"sg_p\", \"top_10\", \"win\"]]\n        .head(n)\n    )\n\nshow_players(pga_clusters, 0, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 1, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 2, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 3, sort_col=\"sg_p\", n=20)\n```\n\n### Display distinct three names for each clusters as an example\n\n```{python}\nfrom IPython.display import display\n\ndef pick_players(df, cluster_id, n=3):\n    cluster_df = (\n        df[df[\"cluster\"] == cluster_id]\n        .sort_values([\"year\", \"scoring\"])\n        [[\"name\", \"year\", \"scoring\", \"sg_ttg\", \"sg_p\", \"top_10\", \"win\"]]\n        .drop_duplicates(subset=[\"name\"])  \n    )\n    return cluster_df.head(n)\n\nfor c in range(4):\n    print(f\"\\n=== Cluster {c} ===\")\n    display(pick_players(pga_clusters, c, n=3))\n    \n```\n","srcMarkdownNoYaml":"\n\nThe goal of this section is to use **unsupervised learning** to discover natural player “types” on the PGA Tour, based purely on performance statistics rather than predefined labels. Instead of asking “who won?” or “who scored lowest?” we ask:\n\n> Are there identifiable types of PGA Tour players using unsupervised learning?\n\nTo answer this, I apply K-Means clustering to season-level player performance and then interpret the resulting clusters using both summary statistics and example players from each group.\n\n### Data Import\n\n```{python}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npga = pd.read_csv(\"data/processed-data/pga_cleaned.csv\")\npga.head()\n```\n\n### Filter Features\n\nClustering performance profiles requires choosing a set of features that capture how players **actually play golf**, not just whether they happen to win in a given season. For this reason, I include the following variables:\n\n-   **scoring** – scoring average (strokes per round)\n\n-   **drive_distance** – average driving distance\n\n-   **gir_pct** – greens in regulation percentage\n\n-   **sg_p** – strokes gained putting\n\n-   **sg_ttg** – strokes gained tee-to-green\n\n-   **sg_t** – strokes gained total\n\n-   **top_10** – number of top-10 finishes in the season\n\n-   **win** – number of wins in the season\n\nTogether, these variables summarize scoring outcome, long-game and short-game performance, and high-end results (top-10s and wins).\n\nBefore fitting the model, I drop any rows with missing values in these columns to ensure the clustering algorithm has complete data for each player-season.\n\n```{python}\nfeatures = [\n    \"scoring\",\n    \"drive_distance\",\n    \"gir_pct\",\n    \"sg_p\",\n    \"sg_ttg\",\n    \"sg_t\",\n    \"top_10\",\n    \"win\"\n]\n\nX = pga[features].dropna()\nX.head()\n```\n\n### Scale\n\nBecause these features are on different scales (for example, scoring is around 70, sg_ttg tends to be between roughly −3 and +3, and top_10 is a small integer), I standardize all selected features using StandardScaler.\n\nStandardization transforms each variable to have:\n\n-   Mean ≈ 0\n\n    Standard deviation ≈ 1\n\nThis is especially important for K-Means, which uses Euclidean distance in feature space. Without scaling, variables with larger numerical ranges (like scoring or driving distance) would dominate the distance computation and overshadow more subtle but important statistics like strokes gained.\n\n```{python}\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n\n### Calculate optimal K mean\n\nTo determine a reasonable number of clusters, I use the Elbow Method.\n\nFor k from 2 to 9, I:\n\n1.  Fit a K-Means model with `n_clusters = k` on the scaled data\n\n2.  Record the **inertia** (within-cluster sum of squared distances)\n\nPlotting inertia against k typically shows a rapidly decreasing curve that then begins to “bend” or “flatten” — the “elbow.” That elbow represents a good trade-off between model complexity (more clusters) and compactness (lower within-cluster variance).\n\nIn this case, the elbow occurs around k = 4, suggesting that four distinct performance archetypes are present in the data. This choice balances interpretability and segmentation detail.\n\n```{python}\ninertia = []\nK_range = range(2, 10)\n\nfor k in K_range:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X_scaled)\n    inertia.append(km.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.plot(K_range, inertia, marker='o')\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Inertia (Within-Cluster SSE)\")\nplt.title(\"Elbow Method for Optimal k\")\nplt.grid(True)\nplt.show()\n\n```\n\n### Set K mean as 4\n\nAfter selecting **k = 4**, I fit a K-Means model with:\n\n-   `n_clusters = 4`\n\n-   `random_state = 42`\n\n-   `n_init = 10`\n\nThe model assigns each player-season to one of four clusters, stored in a new column `cluster`. I then create a `pga_clusters` dataframe, which contains all original variables for the filtered rows along with the cluster label.\n\nThis allows me to:\n\n-   Summarize each cluster by averaging statistics\n\n-   Look up specific players in each cluster\n\n-   Cross-reference cluster labels with scoring, wins, and strokes-gained metrics\n\n```{python}\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_scaled)\n\npga_clusters = pga.loc[X.index].copy()\npga_clusters[\"cluster\"] = clusters\n```\n\n### PCA graph\n\nTo understand how the clusters are arranged in high-dimensional space, I apply Principal Component Analysis (PCA) to reduce the standardized feature space to two components (PC1 and PC2).\n\nPCA serves two purposes here:\n\n1.  It compresses information from 8 features into 2 dimensions for visualization.\n\n2.  It preserves as much variance as possible, so the clusters remain meaningfully separated in the 2D plot.\n\nEach player-season is projected onto the (PC1, PC2) plane, and the points are colored by cluster label. The resulting scatterplot:\n\n-   Shows clearly separated groupings, confirming that the K-Means clustering captures real structure in the data.\n\n-   Provides a visual intuition for how different performance profiles occupy different regions of the feature space (e.g., elite ball-strikers vs. struggling players).\n\n```{python}\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\ncoords = pca.fit_transform(X_scaled)\n\npga_clusters[\"PC1\"] = coords[:,0]\npga_clusters[\"PC2\"] = coords[:,1]\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(\n    data=pga_clusters,\n    x=\"PC1\", y=\"PC2\",\n    hue=\"cluster\", palette=\"tab10\", alpha=0.6\n)\nplt.title(\"PCA Visualization of Player Clusters\")\nplt.savefig(\"images/pca_clusters.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n```\n\n### Cluster Profile\n\n**Cluster 0 - Balanced Professionals**\n\n-   Average SG metrics\n\n-   Solid but not outstanding in any single category\n\n-   Mid-level scoring average\n\n**Cluster 1 - Struggling Players**\n\n-   Negative SG_TTG and SG_T\n\n-   Higher scoring average\n\n-   Few top-10 finishes\n\n**Cluster 2 - Elite Ball Strikers**\n\n-   Highest SG_TTG\n\n-   Lowest scoring average\n\n-   Best combination of GIR%, distance, and SG_TTG\n\n-   Most number of wins and top-10s\n\n**Cluster 3 - Elite Putters**\n\n-   Highest SG_P\n\n-   Moderate SG_TTG\n\n-   Relies heavily on short-game performance\n\n```{python}\ncluster_profile = pga_clusters.groupby(\"cluster\")[features].mean().round(2)\ncluster_profile\n```\n\n### Cluster player names\n\nTo make the clusters more concrete and interpretable, I display a few representative players for each cluster.\n\nFor each cluster:\n\n1.  Filter by cluster label\n\n2.  Sort by year and scoring\n\n3.  Drop duplicate names so each player appears once\n\n4.  Show a small number of players with their scoring, SG_TTG, SG_P, top-10s, and wins\n\nThis yields intuitive examples such as:\n\n-   **Cluster 0 (Balanced Professionals):** players like Anthony Kim, Jeff Overton, Kenny Perry\n\n-   **Cluster 1 (Struggling Players):** players like Anders Hansen, Glen Day, Gavin Coles\n\n-   **Cluster 2 (Elite Ball-Strikers):** players like Tiger Woods, Ernie Els, Justin Rose\n\n-   **Cluster 3 (Elite Putters):** players like Arron Oberholser, Padraig Harrington, Tim Clark\n\nBy combining cluster averages and named examples, the analysis shows that unsupervised learning successfully recovers meaningful player archetypes that correspond to real-world styles and reputations on the PGA Tour.\n\n```{python}\npga_clusters = pga.loc[X.index].copy()\npga_clusters[\"cluster\"] = clusters\n\n\ndef show_players(df, cluster_id, sort_col=\"scoring\", n=15):\n    return (\n        df[df[\"cluster\"] == cluster_id]\n        .sort_values(sort_col)\n        [[\"name\", \"year\", \"scoring\", \"sg_ttg\", \"sg_p\", \"top_10\", \"win\"]]\n        .head(n)\n    )\n\nshow_players(pga_clusters, 0, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 1, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 2, sort_col=\"scoring\", n=20)\nshow_players(pga_clusters, 3, sort_col=\"sg_p\", n=20)\n```\n\n### Display distinct three names for each clusters as an example\n\n```{python}\nfrom IPython.display import display\n\ndef pick_players(df, cluster_id, n=3):\n    cluster_df = (\n        df[df[\"cluster\"] == cluster_id]\n        .sort_values([\"year\", \"scoring\"])\n        [[\"name\", \"year\", \"scoring\", \"sg_ttg\", \"sg_p\", \"top_10\", \"win\"]]\n        .drop_duplicates(subset=[\"name\"])  \n    )\n    return cluster_df.head(n)\n\nfor c in range(4):\n    print(f\"\\n=== Cluster {c} ===\")\n    display(pick_players(pga_clusters, c, n=3))\n    \n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"embed-resources":true,"output-file":"unsupervised.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":"cosmo","smooth-scroll":true,"title":"Unsupervised Learning","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}