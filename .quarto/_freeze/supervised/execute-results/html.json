{
  "hash": "edee92bd60809b737ed5118544976ba2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Supervised Learning\"\nformat: \n  html:\n    toc: true\n    code-fold: true\n    embed-resources: true\neditor: visual\n---\n\n### Supervised Learning\n\n### Setup\n\n::: {#9561c2ab .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import root_mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import (\n    mean_squared_error,\n    r2_score,\n    accuracy_score,\n    roc_auc_score,\n    classification_report,\n)\n```\n:::\n\n\n### Load data\n\n::: {#d10ef337 .cell execution_count=2}\n``` {.python .cell-code}\npga = pd.read_csv(\"data/processed-data/pga_cleaned.csv\")\npga.head()\npga.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nIndex(['name', 'year', 'country', 'scoring', 'drive_distance', 'gir_pct',\n       'sg_p', 'sg_ttg', 'sg_t', 'top_10', 'win'],\n      dtype='object')\n```\n:::\n:::\n\n\n### Regression (Predict Score)\n\n::: {#21457d6b .cell execution_count=3}\n``` {.python .cell-code}\nreg_features = [\n  \"drive_distance\",\n  \"gir_pct\",\n  \"sg_p\",\n  \"sg_ttg\",\n]\nreg_target = \"scoring\"\n\nreg_df = pga[reg_features + [reg_target]].dropna()\nX_reg = reg_df[reg_features]\ny_reg = reg_df[reg_target]\n\nX_reg.shape, y_reg.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n((2747, 4), (2747,))\n```\n:::\n:::\n\n\n### Training and testing split (Predict Score)\n\n::: {#2659206f .cell execution_count=4}\n``` {.python .cell-code}\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\nX_reg, y_reg, test_size=0.2, random_state=42\n)\n\nscaler_reg = StandardScaler()\nX_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\nX_test_reg_scaled = scaler_reg.transform(X_test_reg)\n```\n:::\n\n\n### Linear Regression (Predict Score)\n\n::: {#0761d839 .cell execution_count=5}\n``` {.python .cell-code}\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_reg_scaled, y_train_reg)\n\ny_pred_lin = lin_reg.predict(X_test_reg_scaled)\n\nrmse_lin = root_mean_squared_error(y_test_reg, y_pred_lin)\nr2_lin = r2_score(y_test_reg, y_pred_lin)\n\nprint(\"Linear Regression (Predict Score)\")\nprint(\" RMSE:\", rmse_lin)\nprint(\" R^2 :\", r2_lin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression (Predict Score)\n RMSE: 0.1919919438714076\n R^2 : 0.9098003470553254\n```\n:::\n:::\n\n\n### Random Forest (Predict Score)\n\n::: {#01c7d39e .cell execution_count=6}\n``` {.python .cell-code}\nrf_reg = RandomForestRegressor(\n  n_estimators=300,\n  random_state=42,\n  max_depth=None,\n  n_jobs=-1\n)\nrf_reg.fit(X_train_reg, y_train_reg)\n\ny_pred_rf = rf_reg.predict(X_test_reg)\n\nrmse_rf = root_mean_squared_error(y_test_reg, y_pred_rf)\nr2_rf = r2_score(y_test_reg, y_pred_rf)\n\nprint(\"Random Forest Regression (Predict Score)\")\nprint(\"  RMSE:\", rmse_rf)\nprint(\"  R^2 :\", r2_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Regression (Predict Score)\n  RMSE: 0.2057418971539437\n  R^2 : 0.8964179897284077\n```\n:::\n:::\n\n\n### Feature importance (Predict Score)\n\n::: {#d07c6b16 .cell execution_count=7}\n``` {.python .cell-code}\nimportances_reg = pd.Series(\nrf_reg.feature_importances_,\nindex=reg_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_reg.values, y=importances_reg.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Score)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_scoring.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_reg\n```\n\n::: {.cell-output .cell-output-display}\n![](supervised_files/figure-html/cell-8-output-1.png){width=757 height=468}\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nsg_ttg            0.726565\nsg_p              0.233879\ngir_pct           0.020768\ndrive_distance    0.018789\ndtype: float64\n```\n:::\n:::\n\n\n### Classification Model (Predict Win)\n\n::: {#05fc5b83 .cell execution_count=8}\n``` {.python .cell-code}\npga[\"has_win\"] = (pga[\"win\"] > 0).astype(int)\n\nclf_features = [\n\"drive_distance\",\n\"gir_pct\",\n\"sg_p\",\n\"sg_ttg\",\n]\n\nclf_df = pga[clf_features + [\"has_win\"]].dropna()\nX_clf = clf_df[clf_features]\ny_clf = clf_df[\"has_win\"]\n\nX_clf.shape, y_clf.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n((2747, 4),\n has_win\n 0    2410\n 1     337\n Name: count, dtype: int64)\n```\n:::\n:::\n\n\n### Training and testing split (Predict Win)\n\n::: {#75b9c1c6 .cell execution_count=9}\n``` {.python .cell-code}\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\nX_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n)\n\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n```\n:::\n\n\n### Logistic Regression (Predict Win)\n\n::: {#bfa43a96 .cell execution_count=10}\n``` {.python .cell-code}\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train_clf_scaled, y_train_clf)\n\ny_pred_log = log_reg.predict(X_test_clf_scaled)\ny_prob_log = log_reg.predict_proba(X_test_clf_scaled)[:, 1]\n\nacc_log = accuracy_score(y_test_clf, y_pred_log)\nauc_log = roc_auc_score(y_test_clf, y_prob_log)\n\nprint(\"Logistic Regression (Predict Win)\")\nprint(\"  Accuracy:\", acc_log)\nprint(\"  ROC AUC :\", auc_log)\nprint(classification_report(y_test_clf, y_pred_log))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression (Predict Win)\n  Accuracy: 0.8818181818181818\n  ROC AUC : 0.7675906183368869\n              precision    recall  f1-score   support\n\n           0       0.88      1.00      0.94       483\n           1       0.75      0.04      0.08        67\n\n    accuracy                           0.88       550\n   macro avg       0.82      0.52      0.51       550\nweighted avg       0.87      0.88      0.83       550\n\n```\n:::\n:::\n\n\n### Random Forest (Predict Win)\n\n::: {#b165a21b .cell execution_count=11}\n``` {.python .cell-code}\nrf_clf = RandomForestClassifier(\nn_estimators=400,\nrandom_state=42,\nmax_depth=None,\nn_jobs=-1\n)\nrf_clf.fit(X_train_clf, y_train_clf)\n\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_prob_rf_clf = rf_clf.predict_proba(X_test_clf)[:, 1]\n\nacc_rf = accuracy_score(y_test_clf, y_pred_rf_clf)\nauc_rf = roc_auc_score(y_test_clf, y_prob_rf_clf)\n\nprint(\"Random Forest Classifier (Predict Win)\")\nprint(\"  Accuracy:\", acc_rf)\nprint(\"  ROC AUC :\", auc_rf)\nprint(classification_report(y_test_clf, y_pred_rf_clf))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Classifier (Predict Win)\n  Accuracy: 0.88\n  ROC AUC : 0.7098668149933561\n              precision    recall  f1-score   support\n\n           0       0.89      0.99      0.94       483\n           1       0.54      0.10      0.17        67\n\n    accuracy                           0.88       550\n   macro avg       0.71      0.55      0.56       550\nweighted avg       0.85      0.88      0.84       550\n\n```\n:::\n:::\n\n\n#Feature importance (Predict Win)\n\n::: {#7eeb2e46 .cell execution_count=12}\n``` {.python .cell-code}\nimportances_clf = pd.Series(\nrf_clf.feature_importances_,\nindex=clf_features\n).sort_values(ascending=False)\n\nplt.figure(figsize=(8,5))\nsns.barplot(x=importances_clf.values, y=importances_clf.index)\nplt.xlabel(\"Feature Importance\")\nplt.title(\"RF Feature Importance (Predict Win)\")\nplt.tight_layout()\nplt.savefig(\"images/supervised_rf_importance_wins.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\nimportances_clf\n```\n\n::: {.cell-output .cell-output-display}\n![](supervised_files/figure-html/cell-13-output-1.png){width=757 height=468}\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nsg_ttg            0.298724\ngir_pct           0.239733\ndrive_distance    0.231139\nsg_p              0.230404\ndtype: float64\n```\n:::\n:::\n\n\n",
    "supporting": [
      "supervised_files"
    ],
    "filters": [],
    "includes": {}
  }
}